{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "authorship_tag": "ABX9TyO4fbwGW2WTtAtfaQWXsBXJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Spykabore15/students_RAG_project/blob/main/RAG_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mount Drive and Install Dependencies"
      ],
      "metadata": {
        "id": "UFSS07eYLleY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AbWzdgjzAesk"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install -q \\\n",
        "  langchain \\\n",
        "  langchain-community==0.2.* \\\n",
        "  faiss-cpu \\\n",
        "  sentence-transformers \\\n",
        "  transformers \\\n",
        "  accelerate \\\n",
        "  pypdf==4.* \\\n",
        "  streamlit \\\n",
        "  pyngrok\n",
        "\n",
        "print(\"âœ… All dependencies installed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set up ngrok"
      ],
      "metadata": {
        "id": "4oqa7JPlLhf9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up ngrok\n",
        "from pyngrok import ngrok\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Get ngrok token from secrets\n",
        "ngrok_token = userdata.get('NGROK_AUTH_KEY')\n",
        "ngrok.set_auth_token(ngrok_token)\n"
      ],
      "metadata": {
        "id": "I8Ea-O3sApkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Streamit app configuration"
      ],
      "metadata": {
        "id": "ij6yy_CHLqlD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/app.py\n",
        "import streamlit as st\n",
        "from pathlib import Path\n",
        "import json\n",
        "from typing import List, Tuple\n",
        "import torch\n",
        "\n",
        "# Langchain imports\n",
        "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain import HuggingFacePipeline\n",
        "from langchain.prompts import PromptTemplate\n",
        "from transformers import pipeline\n",
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "# Page config\n",
        "st.set_page_config(\n",
        "    page_title=\"Enhanced Student RAG Chatbot\",\n",
        "    page_icon=\"ðŸŽ“\",\n",
        "    layout=\"wide\"\n",
        ")\n",
        "\n",
        "# Constants\n",
        "DATA_PATH = \"/content/drive/MyDrive/TRAINING DATA\"\n",
        "\n",
        "# ==================== QUERY DECOMPOSITION ====================\n",
        "\n",
        "DECOMPOSITION_PROMPT = \"\"\"You are a query analyzer. Analyze if this question needs to be broken down into sub-questions.\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Rules:\n",
        "1. If the question is simple and focused, respond with: SIMPLE\n",
        "2. If the question has multiple parts or requires information from different sources, decompose it into 2-4 sub-questions.\n",
        "\n",
        "Response format for complex questions:\n",
        "SUB-QUERY 1: [first sub-question]\n",
        "SUB-QUERY 2: [second sub-question]\n",
        "SUB-QUERY 3: [third sub-question] (if needed)\n",
        "\n",
        "Examples:\n",
        "\n",
        "Question: \"Who is Dr. Sarah Chen?\"\n",
        "Response: SIMPLE\n",
        "\n",
        "Question: \"Which students worked with Dr. Sarah Chen on AI healthcare projects and what were their evaluation scores?\"\n",
        "Response:\n",
        "SUB-QUERY 1: Which students worked with Dr. Sarah Chen?\n",
        "SUB-QUERY 2: What AI healthcare projects exist in the database?\n",
        "SUB-QUERY 3: What were the evaluation scores for these students?\n",
        "\n",
        "Question: \"Compare robotics projects with environmental science projects\"\n",
        "Response:\n",
        "SUB-QUERY 1: What robotics projects are in the database?\n",
        "SUB-QUERY 2: What environmental science projects are in the database?\n",
        "\n",
        "Now analyze this question:\n",
        "Question: {question}\n",
        "Response:\"\"\"\n",
        "\n",
        "def decompose_query(llm, question: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Use LLM to decide if decomposition is needed\n",
        "    Returns: list of queries (original or decomposed)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        prompt = DECOMPOSITION_PROMPT.format(question=question)\n",
        "        response = llm.predict(prompt)\n",
        "\n",
        "        # Parse response\n",
        "        if \"SIMPLE\" in response:\n",
        "            return [question]\n",
        "\n",
        "        # Extract sub-queries\n",
        "        sub_queries = []\n",
        "        for line in response.split('\\n'):\n",
        "            if line.strip().startswith('SUB-QUERY'):\n",
        "                # Extract query after the colon\n",
        "                query = line.split(':', 1)[1].strip()\n",
        "                if query:\n",
        "                    sub_queries.append(query)\n",
        "\n",
        "        # Fallback: if parsing failed, use original\n",
        "        return sub_queries if sub_queries else [question]\n",
        "    except Exception as e:\n",
        "        st.warning(f\"Decomposition failed: {e}. Using original query.\")\n",
        "        return [question]\n",
        "\n",
        "# ==================== RERANKING ====================\n",
        "\n",
        "class CrossEncoderReranker:\n",
        "    \"\"\"Wrapper for cross-encoder reranking\"\"\"\n",
        "\n",
        "    def __init__(self, model_name=\"cross-encoder/ms-marco-MiniLM-L-6-v2\", device=\"cuda\"):\n",
        "        self.device = device\n",
        "        self.model = CrossEncoder(model_name, device=device)\n",
        "\n",
        "    def rerank(self, query: str, documents: List[Document], top_k: int = 4) -> List[Tuple[Document, float]]:\n",
        "        \"\"\"\n",
        "        Rerank documents based on relevance to query\n",
        "\n",
        "        Args:\n",
        "            query: User question\n",
        "            documents: List of Document objects\n",
        "            top_k: Number of top documents to return\n",
        "\n",
        "        Returns:\n",
        "            List of (document, score) tuples, sorted by score\n",
        "        \"\"\"\n",
        "        if not documents:\n",
        "            return []\n",
        "\n",
        "        # Prepare pairs for scoring\n",
        "        pairs = [[query, doc.page_content] for doc in documents]\n",
        "\n",
        "        # Get relevance scores\n",
        "        scores = self.model.predict(pairs)\n",
        "\n",
        "        # Combine documents with scores\n",
        "        doc_scores = list(zip(documents, scores))\n",
        "\n",
        "        # Sort by score (descending)\n",
        "        doc_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        return doc_scores[:top_k]\n",
        "\n",
        "# ==================== ENHANCED RAG PIPELINE ====================\n",
        "\n",
        "class EnhancedRAGPipeline:\n",
        "    \"\"\"Complete RAG pipeline with decomposition and reranking\"\"\"\n",
        "\n",
        "    def __init__(self, vectorstore, llm, reranker, retrieval_k=20, final_k=4):\n",
        "        self.vectorstore = vectorstore\n",
        "        self.llm = llm\n",
        "        self.reranker = reranker\n",
        "        self.retrieval_k = retrieval_k\n",
        "        self.final_k = final_k\n",
        "\n",
        "        # Retriever with higher k for reranking\n",
        "        self.retriever = vectorstore.as_retriever(\n",
        "            search_kwargs={\"k\": retrieval_k}\n",
        "        )\n",
        "\n",
        "    def retrieve_documents(self, sub_queries: List[str]) -> List[Document]:\n",
        "        \"\"\"\n",
        "        Retrieve documents for all sub-queries and deduplicate\n",
        "        \"\"\"\n",
        "        all_docs = []\n",
        "\n",
        "        # Retrieve for each sub-query\n",
        "        for sub_q in sub_queries:\n",
        "            docs = self.retriever.get_relevant_documents(sub_q)\n",
        "            all_docs.extend(docs)\n",
        "\n",
        "        # Deduplicate by content hash\n",
        "        unique_docs = []\n",
        "        seen_content = set()\n",
        "        for doc in all_docs:\n",
        "            content_hash = hash(doc.page_content)\n",
        "            if content_hash not in seen_content:\n",
        "                unique_docs.append(doc)\n",
        "                seen_content.add(content_hash)\n",
        "\n",
        "        return unique_docs\n",
        "\n",
        "    def generate_answer(self, question: str, context_docs: List[Tuple[Document, float]]) -> str:\n",
        "        \"\"\"\n",
        "        Generate final answer using LLM\n",
        "        \"\"\"\n",
        "        # Format context with sources\n",
        "        context = \"\\n\\n\".join([\n",
        "            f\"[Source {i+1} - Relevance: {score:.2f}]\\n{doc.page_content}\"\n",
        "            for i, (doc, score) in enumerate(context_docs)\n",
        "        ])\n",
        "\n",
        "        prompt = f\"\"\"You are a helpful assistant that answers using only the provided context.\n",
        "If the answer is not contained in the context, say \"I do not know\".\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer (be concise, structured, and cite sources when relevant):\"\"\"\n",
        "\n",
        "        answer = self.llm.predict(prompt)\n",
        "        return answer\n",
        "\n",
        "    def query(self, question: str, enable_decomposition: bool = True, enable_reranking: bool = True):\n",
        "        \"\"\"\n",
        "        Full RAG pipeline with all enhancements\n",
        "\n",
        "        Returns:\n",
        "            dict with answer, sub_queries, sources, and metadata\n",
        "        \"\"\"\n",
        "        # Step 1: Query Decomposition\n",
        "        if enable_decomposition:\n",
        "            sub_queries = decompose_query(self.llm, question)\n",
        "        else:\n",
        "            sub_queries = [question]\n",
        "\n",
        "        # Step 2: Retrieve documents\n",
        "        retrieved_docs = self.retrieve_documents(sub_queries)\n",
        "\n",
        "        # Step 3: Rerank documents\n",
        "        if enable_reranking and retrieved_docs:\n",
        "            # Use original question for reranking (not sub-queries)\n",
        "            reranked_docs = self.reranker.rerank(\n",
        "                query=question,\n",
        "                documents=retrieved_docs,\n",
        "                top_k=self.final_k\n",
        "            )\n",
        "        else:\n",
        "            # No reranking, just take top k\n",
        "            reranked_docs = [(doc, 1.0) for doc in retrieved_docs[:self.final_k]]\n",
        "\n",
        "        # Step 4: Generate answer\n",
        "        answer = self.generate_answer(question, reranked_docs)\n",
        "\n",
        "        return {\n",
        "            \"answer\": answer,\n",
        "            \"sub_queries\": sub_queries,\n",
        "            \"sources\": reranked_docs,\n",
        "            \"num_retrieved\": len(retrieved_docs),\n",
        "            \"num_sources\": len(reranked_docs)\n",
        "        }\n",
        "\n",
        "# ==================== DOCUMENT LOADING (CACHED) ====================\n",
        "\n",
        "@st.cache_resource\n",
        "def load_core_pdfs(data_path: Path) -> List[Document]:\n",
        "    \"\"\"Load core PDF documents with caching\"\"\"\n",
        "    core_pdfs = [\n",
        "        data_path / \"Projects.pdf\",\n",
        "        data_path / \"criteria.pdf\",\n",
        "        data_path / \"Mentors.pdf\"\n",
        "    ]\n",
        "\n",
        "    docs = []\n",
        "    for pdf in core_pdfs:\n",
        "        if pdf.exists():\n",
        "            pages = PyPDFLoader(str(pdf)).load()\n",
        "            for p in pages:\n",
        "                p.metadata.update({\"source\": pdf.name, \"category\": \"core\"})\n",
        "            docs.extend(pages)\n",
        "    return docs\n",
        "\n",
        "@st.cache_resource\n",
        "def load_student_dirs(data_path: Path) -> List[Document]:\n",
        "    \"\"\"Load student directories with caching\"\"\"\n",
        "    docs = []\n",
        "    students_dir = data_path / \"students\"\n",
        "\n",
        "    if not students_dir.exists():\n",
        "        return docs\n",
        "\n",
        "    for student_dir in students_dir.iterdir():\n",
        "        if not student_dir.is_dir():\n",
        "            continue\n",
        "\n",
        "        # Load student report\n",
        "        report_pdf = student_dir / \"report.pdf\"\n",
        "        if report_pdf.exists():\n",
        "            pages = PyPDFLoader(str(report_pdf)).load()\n",
        "            for p in pages:\n",
        "                p.metadata.update({\n",
        "                    \"source\": f\"{student_dir.name}/report.pdf\",\n",
        "                    \"student\": student_dir.name,\n",
        "                    \"category\": \"student_report\"\n",
        "                })\n",
        "            docs.extend(pages)\n",
        "\n",
        "        # Load student summary\n",
        "        summary_txt = student_dir / \"summary.txt\"\n",
        "        if summary_txt.exists():\n",
        "            tdocs = TextLoader(str(summary_txt), encoding=\"utf-8\").load()\n",
        "            for d in tdocs:\n",
        "                d.metadata.update({\n",
        "                    \"source\": f\"{student_dir.name}/summary.txt\",\n",
        "                    \"student\": student_dir.name,\n",
        "                    \"category\": \"student_summary\"\n",
        "                })\n",
        "            docs.extend(tdocs)\n",
        "\n",
        "        # Load student metadata\n",
        "        meta_json = student_dir / \"metadata.json\"\n",
        "        if meta_json.exists():\n",
        "            try:\n",
        "                meta = json.loads(meta_json.read_text(encoding=\"utf-8\"))\n",
        "                meta_doc = Document(\n",
        "                    page_content=json.dumps(meta, ensure_ascii=False, indent=2),\n",
        "                    metadata={\n",
        "                        \"source\": f\"{student_dir.name}/metadata.json\",\n",
        "                        \"student\": student_dir.name,\n",
        "                        \"category\": \"student_metadata\"\n",
        "                    }\n",
        "                )\n",
        "                docs.append(meta_doc)\n",
        "            except Exception as e:\n",
        "                st.warning(f\"Couldn't parse {meta_json}: {e}\")\n",
        "\n",
        "    return docs\n",
        "\n",
        "# ==================== MAIN INITIALIZATION (CACHED) ====================\n",
        "\n",
        "@st.cache_resource\n",
        "def initialize_enhanced_rag():\n",
        "    \"\"\"Initialize the complete enhanced RAG system with caching\"\"\"\n",
        "\n",
        "    # Check GPU availability\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    st.info(f\"ðŸ–¥ï¸ Using device: **{device}**\")\n",
        "\n",
        "    if device == \"cpu\":\n",
        "        st.warning(\"âš ï¸ GPU not detected! This will be slower. Enable GPU in Colab Runtime settings.\")\n",
        "\n",
        "    with st.spinner(\"ðŸ“š Loading documents...\"):\n",
        "        data_path = Path(DATA_PATH)\n",
        "        raw_docs = load_core_pdfs(data_path) + load_student_dirs(data_path)\n",
        "        st.success(f\"âœ… Loaded **{len(raw_docs)}** documents\")\n",
        "\n",
        "    with st.spinner(\" Splitting documents into chunks...\"):\n",
        "        splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=1000,\n",
        "            chunk_overlap=150,\n",
        "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        "        )\n",
        "        chunks = splitter.split_documents(raw_docs)\n",
        "        st.success(f\"âœ… Created **{len(chunks)}** chunks\")\n",
        "\n",
        "    with st.spinner(\"Loading embedding model...\"):\n",
        "        embeddings = HuggingFaceEmbeddings(\n",
        "            model_name=\"all-MiniLM-L6-v2\",\n",
        "            model_kwargs={'device': device}\n",
        "        )\n",
        "        st.success(\"âœ… Embeddings model loaded\")\n",
        "\n",
        "    with st.spinner(\" Building vector store...\"):\n",
        "        vectorstore = FAISS.from_documents(\n",
        "            documents=chunks,\n",
        "            embedding=embeddings\n",
        "        )\n",
        "        st.success(\"âœ… Vector store created\")\n",
        "\n",
        "    with st.spinner(\"ðŸ¤– Loading language model (Mistral-7B)...\"):\n",
        "        pipe = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "            device=0 if device == \"cuda\" else -1,\n",
        "            torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        "            max_new_tokens=300,\n",
        "            temperature=0.1,\n",
        "            return_full_text=False\n",
        "        )\n",
        "        llm = HuggingFacePipeline(pipeline=pipe)\n",
        "        st.success(f\"âœ… Language model loaded on **{device}**\")\n",
        "\n",
        "    with st.spinner(\"ðŸŽ¯ Loading reranker (Cross-Encoder)...\"):\n",
        "        reranker = CrossEncoderReranker(device=device)\n",
        "        st.success(\"âœ… Reranker loaded\")\n",
        "\n",
        "    # Create enhanced RAG pipeline\n",
        "    rag = EnhancedRAGPipeline(\n",
        "        vectorstore=vectorstore,\n",
        "        llm=llm,\n",
        "        reranker=reranker,\n",
        "        retrieval_k=20,\n",
        "        final_k=4\n",
        "    )\n",
        "\n",
        "    st.success(\" **Enhanced RAG system initialized!**\")\n",
        "\n",
        "    return rag\n",
        "\n",
        "# ==================== STREAMLIT UI ====================\n",
        "\n",
        "def main():\n",
        "    st.title(\"ðŸŽ“ Enhanced Student Project RAG Chatbot\")\n",
        "    st.markdown(\"\"\"\n",
        "    Ask questions about student projects, mentors, and evaluation criteria.\n",
        "    **New Features:** Query decomposition + Advanced reranking\n",
        "    \"\"\")\n",
        "\n",
        "    # Sidebar configuration\n",
        "    with st.sidebar:\n",
        "        st.header(\"âš™ï¸ Configuration\")\n",
        "\n",
        "        enable_decomposition = st.checkbox(\n",
        "            \"Enable Query Decomposition\",\n",
        "            value=True,\n",
        "            help=\"Break complex questions into simpler sub-queries\"\n",
        "        )\n",
        "\n",
        "        enable_reranking = st.checkbox(\n",
        "            \"Enable Reranking\",\n",
        "            value=True,\n",
        "            help=\"Reorder retrieved documents by relevance\"\n",
        "        )\n",
        "\n",
        "        show_details = st.checkbox(\n",
        "            \"Show Processing Details\",\n",
        "            value=True,\n",
        "            help=\"Display sub-queries and relevance scores\"\n",
        "        )\n",
        "\n",
        "        st.markdown(\"---\")\n",
        "        st.markdown(\"### ðŸ’¡ Example Questions\")\n",
        "        st.markdown(\"\"\"\n",
        "        **Simple:**\n",
        "        - Who are the mentors for robotics projects?\n",
        "        - What is Dr. Sarah Chen's expertise?\n",
        "\n",
        "        **Complex (will decompose):**\n",
        "        - Which students worked with Dr. Chen on AI healthcare projects and what were their scores?\n",
        "        - Compare robotics projects with environmental science projects\n",
        "        - List students who can collaborate on Robot and Air Quality projects\n",
        "        \"\"\")\n",
        "\n",
        "        st.markdown(\"---\")\n",
        "        st.markdown(\"### System Info\")\n",
        "        if torch.cuda.is_available():\n",
        "            st.success(f\"âœ… GPU: {torch.cuda.get_device_name(0)}\")\n",
        "        else:\n",
        "            st.error(\"âŒ No GPU detected\")\n",
        "\n",
        "    # Initialize RAG system\n",
        "    try:\n",
        "        rag = initialize_enhanced_rag()\n",
        "    except Exception as e:\n",
        "        st.error(f\"âŒ Error initializing RAG system: {e}\")\n",
        "        st.stop()\n",
        "\n",
        "    # Chat interface\n",
        "    if \"messages\" not in st.session_state:\n",
        "        st.session_state.messages = []\n",
        "\n",
        "    # Display chat history\n",
        "    for message in st.session_state.messages:\n",
        "        with st.chat_message(message[\"role\"]):\n",
        "            st.markdown(message[\"content\"])\n",
        "\n",
        "            # Show details if available\n",
        "            if \"details\" in message and show_details:\n",
        "                with st.expander(\"ðŸ” Processing Details\"):\n",
        "                    st.markdown(message[\"details\"])\n",
        "\n",
        "            # Show sources if available\n",
        "            if \"sources\" in message:\n",
        "                with st.expander(\"ðŸ“š Source Documents\"):\n",
        "                    st.markdown(message[\"sources\"])\n",
        "\n",
        "    # Chat input\n",
        "    if question := st.chat_input(\"Ask a question about student projects...\"):\n",
        "        # Add user message\n",
        "        st.session_state.messages.append({\"role\": \"user\", \"content\": question})\n",
        "        with st.chat_message(\"user\"):\n",
        "            st.markdown(question)\n",
        "\n",
        "        # Get response\n",
        "        with st.chat_message(\"assistant\"):\n",
        "            with st.spinner(\"ðŸ¤” Processing your question...\"):\n",
        "                try:\n",
        "                    # Query the RAG system\n",
        "                    result = rag.query(\n",
        "                        question=question,\n",
        "                        enable_decomposition=enable_decomposition,\n",
        "                        enable_reranking=enable_reranking\n",
        "                    )\n",
        "\n",
        "                    # Display answer\n",
        "                    st.markdown(result[\"answer\"])\n",
        "\n",
        "                    # Format processing details\n",
        "                    details_text = \"\"\n",
        "                    if show_details:\n",
        "                        if len(result[\"sub_queries\"]) > 1:\n",
        "                            details_text += \"**ðŸ” Query Decomposition:**\\n\"\n",
        "                            for i, sq in enumerate(result[\"sub_queries\"], 1):\n",
        "                                details_text += f\"{i}. {sq}\\n\"\n",
        "                            details_text += \"\\n\"\n",
        "\n",
        "                        details_text += f\"**Retrieval Stats:**\\n\"\n",
        "                        details_text += f\"- Retrieved: {result['num_retrieved']} documents\\n\"\n",
        "                        details_text += f\"- After reranking: {result['num_sources']} documents\\n\"\n",
        "\n",
        "                        with st.expander(\"ðŸ” Processing Details\"):\n",
        "                            st.markdown(details_text)\n",
        "\n",
        "                    # Format sources\n",
        "                    sources_text = \"\"\n",
        "                    for i, (doc, score) in enumerate(result[\"sources\"], 1):\n",
        "                        source = doc.metadata.get('source', 'Unknown')\n",
        "                        category = doc.metadata.get('category', 'N/A')\n",
        "                        student = doc.metadata.get('student', 'N/A')\n",
        "\n",
        "                        sources_text += f\"**{i}. {source}** (Score: {score:.3f})\\n\"\n",
        "                        sources_text += f\"   - Category: {category}\\n\"\n",
        "                        if student != 'N/A':\n",
        "                            sources_text += f\"   - Student: {student}\\n\"\n",
        "                        sources_text += f\"   - Content preview: {doc.page_content[:200]}...\\n\\n\"\n",
        "\n",
        "                    with st.expander(\"ðŸ“š Source Documents\"):\n",
        "                        st.markdown(sources_text)\n",
        "\n",
        "                    # Add to chat history\n",
        "                    st.session_state.messages.append({\n",
        "                        \"role\": \"assistant\",\n",
        "                        \"content\": result[\"answer\"],\n",
        "                        \"details\": details_text if show_details else None,\n",
        "                        \"sources\": sources_text\n",
        "                    })\n",
        "\n",
        "                except Exception as e:\n",
        "                    error_msg = f\"âŒ Error generating response: {str(e)}\"\n",
        "                    st.error(error_msg)\n",
        "                    st.session_state.messages.append({\n",
        "                        \"role\": \"assistant\",\n",
        "                        \"content\": error_msg\n",
        "                    })\n",
        "\n",
        "    # Action buttons in sidebar\n",
        "    with st.sidebar:\n",
        "        st.markdown(\"---\")\n",
        "        col1, col2 = st.columns(2)\n",
        "\n",
        "        with col1:\n",
        "            if st.button(\"ðŸ—‘ï¸ Clear Chat\", use_container_width=True):\n",
        "                st.session_state.messages = []\n",
        "                st.rerun()\n",
        "\n",
        "        with col2:\n",
        "            if st.button(\"ðŸ”„ Reload System\", use_container_width=True):\n",
        "                st.cache_resource.clear()\n",
        "                st.rerun()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "5_AWoF4lAtUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optional - Pre-build and Save Vector Store\n",
        "\n",
        "Run this once to save time"
      ],
      "metadata": {
        "id": "rhPcUHToJDfR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import torch\n",
        "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "import json\n",
        "\n",
        "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
        "\n",
        "DATA_PATH = \"/content/drive/MyDrive/TRAINING DATA\"\n",
        "VECTORSTORE_PATH = \"/content/drive/MyDrive/vectorstore_enhanced\"\n",
        "\n",
        "def load_all_documents():\n",
        "    \"\"\"Load all documents from the data directory\"\"\"\n",
        "    data_path = Path(DATA_PATH)\n",
        "    docs = []\n",
        "\n",
        "    # Load core PDFs\n",
        "    core_pdfs = [\n",
        "        data_path / \"Projects.pdf\",\n",
        "        data_path / \"criteria.pdf\",\n",
        "        data_path / \"Mentors.pdf\"\n",
        "    ]\n",
        "\n",
        "    for pdf in core_pdfs:\n",
        "        if pdf.exists():\n",
        "            pages = PyPDFLoader(str(pdf)).load()\n",
        "            for p in pages:\n",
        "                p.metadata.update({\"source\": pdf.name, \"category\": \"core\"})\n",
        "            docs.extend(pages)\n",
        "\n",
        "    # Load student directories\n",
        "    students_dir = data_path / \"students\"\n",
        "    if students_dir.exists():\n",
        "        for student_dir in students_dir.iterdir():\n",
        "            if not student_dir.is_dir():\n",
        "                continue\n",
        "\n",
        "            # Report PDF\n",
        "            report_pdf = student_dir / \"report.pdf\"\n",
        "            if report_pdf.exists():\n",
        "                pages = PyPDFLoader(str(report_pdf)).load()\n",
        "                for p in pages:\n",
        "                    p.metadata.update({\n",
        "                        \"source\": f\"{student_dir.name}/report.pdf\",\n",
        "                        \"student\": student_dir.name,\n",
        "                        \"category\": \"student_report\"\n",
        "                    })\n",
        "                docs.extend(pages)\n",
        "\n",
        "            # Summary TXT\n",
        "            summary_txt = student_dir / \"summary.txt\"\n",
        "            if summary_txt.exists():\n",
        "                tdocs = TextLoader(str(summary_txt), encoding=\"utf-8\").load()\n",
        "                for d in tdocs:\n",
        "                    d.metadata.update({\n",
        "                        \"source\": f\"{student_dir.name}/summary.txt\",\n",
        "                        \"student\": student_dir.name,\n",
        "                        \"category\": \"student_summary\"\n",
        "                    })\n",
        "                docs.extend(tdocs)\n",
        "\n",
        "            # Metadata JSON\n",
        "            meta_json = student_dir / \"metadata.json\"\n",
        "            if meta_json.exists():\n",
        "                try:\n",
        "                    meta = json.loads(meta_json.read_text(encoding=\"utf-8\"))\n",
        "                    meta_doc = Document(\n",
        "                        page_content=json.dumps(meta, ensure_ascii=False, indent=2),\n",
        "                        metadata={\n",
        "                            \"source\": f\"{student_dir.name}/metadata.json\",\n",
        "                            \"student\": student_dir.name,\n",
        "                            \"category\": \"student_metadata\"\n",
        "                        }\n",
        "                    )\n",
        "                    docs.append(meta_doc)\n",
        "                except Exception as e:\n",
        "                    print(f\"âš ï¸ Couldn't parse {meta_json}: {e}\")\n",
        "\n",
        "    return docs\n",
        "\n",
        "# Load documents\n",
        "print(\"ðŸ“š Loading documents...\")\n",
        "raw_docs = load_all_documents()\n",
        "print(f\"âœ… Loaded {len(raw_docs)} documents\")\n",
        "\n",
        "# Split into chunks\n",
        "print(\"Splitting documents...\")\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=150,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        ")\n",
        "chunks = splitter.split_documents(raw_docs)\n",
        "print(f\"âœ… Created {len(chunks)} chunks\")\n",
        "\n",
        "# Create embeddings\n",
        "print(\" Creating embeddings...\")\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"all-MiniLM-L6-v2\",\n",
        "    model_kwargs={'device': device}\n",
        ")\n",
        "print(f\"âœ… Embeddings model loaded on {device}\")\n",
        "\n",
        "# Build vector store\n",
        "print(\"Building vector store (this may take 2-3 minutes)...\")\n",
        "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
        "print(\"âœ… Vector store created!\")\n",
        "\n",
        "# Save to disk\n",
        "print(f\" Saving vector store to {VECTORSTORE_PATH}...\")\n",
        "vectorstore.save_local(VECTORSTORE_PATH)\n",
        "print(\"âœ… Vector store saved! You can now skip this cell in future runs.\")\n"
      ],
      "metadata": {
        "id": "RwjkCIutHWW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run the app with ngrok"
      ],
      "metadata": {
        "id": "UI7UkiWuL0SO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import time\n",
        "import requests\n",
        "\n",
        "# Kill any existing streamlit processes\n",
        "!pkill -f streamlit\n",
        "\n",
        "# Wait a moment for cleanup\n",
        "time.sleep(2)\n",
        "\n",
        "# Start Streamlit in background\n",
        "print(\"Starting Streamlit...\")\n",
        "streamlit_process = subprocess.Popen(\n",
        "    [\"streamlit\", \"run\", \"/content/app.py\",\n",
        "     \"--server.port=8501\",\n",
        "     \"--server.address=localhost\",  # Changed from 0.0.0.0\n",
        "     \"--server.headless=true\"],\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.PIPE\n",
        ")\n",
        "\n",
        "# Wait for Streamlit to actually start (checking if it's ready)\n",
        "print(\"â³ Waiting for Streamlit to initialize...\")\n",
        "max_retries = 5\n",
        "for i in range(max_retries):\n",
        "    try:\n",
        "        response = requests.get(\"http://localhost:8501\")\n",
        "        if response.status_code == 200:\n",
        "            print(\"âœ… Streamlit is ready!\")\n",
        "            break\n",
        "    except:\n",
        "        pass\n",
        "    time.sleep(2)\n",
        "    print(f\"   Attempt {i+1}/{max_retries}...\")\n",
        "else:\n",
        "    print(\"âŒ Streamlit failed to start. Check logs below:\")\n",
        "    print(streamlit_process.stderr.read().decode())\n",
        "    raise Exception(\"Streamlit didn't start properly\")\n",
        "\n",
        "# Now create ngrok tunnel\n",
        "print(\" Creating ngrok tunnel...\")\n",
        "public_url = ngrok.connect(8501)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\" Enhanced RAG Chatbot is LIVE!\")\n",
        "print(\"=\" * 60)\n",
        "print(f\" Public URL: {public_url}\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nâœ… Features Enabled:\")\n",
        "print(\"  - Query Decomposition (LLM-based)\")\n",
        "print(\"  - Cross-Encoder Reranking\")\n",
        "print(\"  - Enhanced Retrieval (k=20 â†’ top 4)\")\n",
        "print(\"  - Source Attribution with Scores\")\n",
        "print(\"\\nâš ï¸ Keep this cell running to maintain the connection\")\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "id": "fX1PYcWIA5w5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "B5TumDKWQXir"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optional - Test the System Directly in Notebook"
      ],
      "metadata": {
        "id": "Ik8XVK8iOjRC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test individual components before running Streamlit\n",
        "\n",
        "from transformers import pipeline\n",
        "from langchain import HuggingFacePipeline\n",
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "# Load models\n",
        "print(\"Loading models for testing...\")\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "    device=device,\n",
        "    torch_dtype=torch.float16 if device >= 0 else torch.float32,\n",
        "    max_new_tokens=100,\n",
        "    return_full_text=False\n",
        ")\n",
        "llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "# Test decomposition\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Testing Query Decomposition\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "test_questions = [\n",
        "    \"Who is Dr. Sarah Chen?\",\n",
        "    \"Which students worked with Dr. Chen and what were their scores?\",\n",
        "    \"Compare robotics and environmental projects\"\n",
        "]\n",
        "\n",
        "DECOMP_PROMPT = \"\"\"You are a query analyzer. Analyze if this question needs to be broken down.\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Rules:\n",
        "1. If simple, respond with: SIMPLE\n",
        "2. If complex, decompose into sub-questions.\n",
        "\n",
        "Response format:\n",
        "SUB-QUERY 1: [question]\n",
        "SUB-QUERY 2: [question]\n",
        "\n",
        "Now analyze:\n",
        "Question: {question}\n",
        "Response:\"\"\"\n",
        "\n",
        "for q in test_questions:\n",
        "    prompt = DECOMP_PROMPT.format(question=q)\n",
        "    response = llm.predict(prompt)\n",
        "    print(f\"\\nQ: {q}\")\n",
        "    print(f\"Response: {response[:200]}\")\n",
        "\n",
        "# Test reranker\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Testing Cross-Encoder Reranker\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
        "\n",
        "query = \"AI healthcare projects\"\n",
        "test_docs = [\n",
        "    \"This project focuses on AI-powered medical diagnosis using deep learning.\",\n",
        "    \"Students worked on environmental monitoring systems.\",\n",
        "    \"The healthcare AI system achieved 95% accuracy in disease prediction.\",\n",
        "    \"Robotics project for autonomous delivery in hospitals.\"\n",
        "]\n",
        "\n",
        "pairs = [[query, doc] for doc in test_docs]\n",
        "scores = reranker.predict(pairs)\n",
        "\n",
        "print(f\"Query: {query}\\n\")\n",
        "for doc, score in sorted(zip(test_docs, scores), key=lambda x: x[1], reverse=True):\n",
        "    print(f\"Score: {score:.3f} - {doc[:60]}...\")\n",
        "\n",
        "print(\"\\nâœ… All components working! Ready to use Streamlit app.\")"
      ],
      "metadata": {
        "id": "HSglcyJRH_Yp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stop Everything and Cleanup\n"
      ],
      "metadata": {
        "id": "t-pCMQfOOZGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this when you're done\n",
        "\n",
        "import os\n",
        "import signal\n",
        "\n",
        "# Kill streamlit\n",
        "!pkill -f streamlit\n",
        "\n",
        "# Kill ngrok tunnels\n",
        "ngrok.disconnect(public_url)\n",
        "\n",
        "print(\"âœ… Streamlit and ngrok stopped\")"
      ],
      "metadata": {
        "id": "nFJ-W97hBB4L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}