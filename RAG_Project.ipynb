{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "authorship_tag": "ABX9TyNle+6SY6/reZaPwiUml0tZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Spykabore15/students_RAG_project/blob/main/RAG_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mount Drive and Install Dependencies"
      ],
      "metadata": {
        "id": "UFSS07eYLleY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbWzdgjzAesk",
        "outputId": "c81e56a7-d403-467d-ff59-021a4cf037e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m91.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m158.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m397.1/397.1 kB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m311.8/311.8 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m104.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m152.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pytensor 2.36.3 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "tobler 0.13.0 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "google-adk 1.21.0 requires tenacity<10.0.0,>=9.0.0, but you have tenacity 8.5.0 which is incompatible.\n",
            "langgraph-prebuilt 1.0.5 requires langchain-core>=1.0.0, but you have langchain-core 0.2.43 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "rasterio 1.5.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install -q \\\n",
        "  langchain \\\n",
        "  langchain-community==0.2.* \\\n",
        "  faiss-cpu \\\n",
        "  sentence-transformers \\\n",
        "  transformers \\\n",
        "  accelerate \\\n",
        "  pypdf==4.* \\\n",
        "  streamlit \\\n",
        "  pyngrok\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set up ngrok"
      ],
      "metadata": {
        "id": "4oqa7JPlLhf9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up ngrok\n",
        "from pyngrok import ngrok\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Get ngrok token from secrets\n",
        "ngrok_token = userdata.get('NGROK_AUTH_KEY')\n",
        "ngrok.set_auth_token(ngrok_token)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8Ea-O3sApkw",
        "outputId": "d424d4fa-66db-4bb0-c5d9-a64118b7d97f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Streamit app configuration"
      ],
      "metadata": {
        "id": "ij6yy_CHLqlD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/app.py\n",
        "import streamlit as st\n",
        "from pathlib import Path\n",
        "import json\n",
        "from typing import List\n",
        "import torch\n",
        "\n",
        "# Langchain imports\n",
        "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain import HuggingFacePipeline\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "from transformers import pipeline\n",
        "\n",
        "# Page config\n",
        "st.set_page_config(\n",
        "    page_title=\"Student RAG Chatbot\",\n",
        "    page_icon=\"ðŸŽ“\",\n",
        "    layout=\"wide\"\n",
        ")\n",
        "\n",
        "# Constants\n",
        "DATA_PATH = \"/content/drive/MyDrive/TRAINING DATA\"\n",
        "\n",
        "@st.cache_resource\n",
        "def load_core_pdfs(data_path: Path) -> List[Document]:\n",
        "    \"\"\"Load core PDF documents with caching\"\"\"\n",
        "    core_pdfs = [\n",
        "        data_path / \"Projects.pdf\",\n",
        "        data_path / \"criteria.pdf\",\n",
        "        data_path / \"Mentors.pdf\"\n",
        "    ]\n",
        "\n",
        "    docs = []\n",
        "    for pdf in core_pdfs:\n",
        "        if pdf.exists():\n",
        "            pages = PyPDFLoader(str(pdf)).load()\n",
        "            for p in pages:\n",
        "                p.metadata.update({\"source\": pdf.name, \"category\": \"core\"})\n",
        "            docs.extend(pages)\n",
        "    return docs\n",
        "\n",
        "@st.cache_resource\n",
        "def load_student_dirs(data_path: Path) -> List[Document]:\n",
        "    \"\"\"Load student directories with caching\"\"\"\n",
        "    docs = []\n",
        "    students_dir = data_path / \"students\"\n",
        "\n",
        "    if not students_dir.exists():\n",
        "        return docs\n",
        "\n",
        "    for student_dir in students_dir.iterdir():\n",
        "        if not student_dir.is_dir():\n",
        "            continue\n",
        "\n",
        "        # Load student report\n",
        "        report_pdf = student_dir / \"report.pdf\"\n",
        "        if report_pdf.exists():\n",
        "            pages = PyPDFLoader(str(report_pdf)).load()\n",
        "            for p in pages:\n",
        "                p.metadata.update({\n",
        "                    \"source\": f\"{student_dir.name}/report.pdf\",\n",
        "                    \"student\": student_dir.name,\n",
        "                    \"category\": \"student_report\"\n",
        "                })\n",
        "            docs.extend(pages)\n",
        "\n",
        "        # Load student summary\n",
        "        summary_txt = student_dir / \"summary.txt\"\n",
        "        if summary_txt.exists():\n",
        "            tdocs = TextLoader(str(summary_txt), encoding=\"utf-8\").load()\n",
        "            for d in tdocs:\n",
        "                d.metadata.update({\n",
        "                    \"source\": f\"{student_dir.name}/summary.txt\",\n",
        "                    \"student\": student_dir.name,\n",
        "                    \"category\": \"student_summary\"\n",
        "                })\n",
        "            docs.extend(tdocs)\n",
        "\n",
        "        # Load student metadata\n",
        "        meta_json = student_dir / \"metadata.json\"\n",
        "        if meta_json.exists():\n",
        "            try:\n",
        "                meta = json.loads(meta_json.read_text(encoding=\"utf-8\"))\n",
        "                meta_doc = Document(\n",
        "                    page_content=json.dumps(meta, ensure_ascii=False, indent=2),\n",
        "                    metadata={\n",
        "                        \"source\": f\"{student_dir.name}/metadata.json\",\n",
        "                        \"student\": student_dir.name,\n",
        "                        \"category\": \"student_metadata\"\n",
        "                    }\n",
        "                )\n",
        "                docs.append(meta_doc)\n",
        "            except Exception as e:\n",
        "                st.warning(f\"Couldn't parse {meta_json}: {e}\")\n",
        "\n",
        "    return docs\n",
        "\n",
        "@st.cache_resource\n",
        "def initialize_rag_chain():\n",
        "    \"\"\"Initialize the complete RAG chain with caching\"\"\"\n",
        "    import torch\n",
        "\n",
        "    # Check GPU availability\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    st.info(f\"Using device: {device}\")\n",
        "\n",
        "    if device == \"cpu\":\n",
        "        st.warning(\"âš ï¸ GPU not detected! This will be very slow. Make sure GPU is enabled in Colab.\")\n",
        "\n",
        "    with st.spinner(\"Loading documents...\"):\n",
        "        # Load documents\n",
        "        data_path = Path(DATA_PATH)\n",
        "        raw_docs = load_core_pdfs(data_path) + load_student_dirs(data_path)\n",
        "        st.success(f\"âœ“ Loaded {len(raw_docs)} documents\")\n",
        "\n",
        "    with st.spinner(\"Splitting documents into chunks...\"):\n",
        "        # Split documents\n",
        "        splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=1000,\n",
        "            chunk_overlap=150,\n",
        "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        "        )\n",
        "        chunks = splitter.split_documents(raw_docs)\n",
        "        st.success(f\"âœ“ Created {len(chunks)} chunks\")\n",
        "\n",
        "    with st.spinner(\"Loading embedding model...\"):\n",
        "        # Initialize embeddings with GPU\n",
        "        embeddings = HuggingFaceEmbeddings(\n",
        "            model_name=\"all-MiniLM-L6-v2\",\n",
        "            model_kwargs={'device': device}\n",
        "        )\n",
        "        st.success(\"âœ“ Embeddings model loaded\")\n",
        "\n",
        "    with st.spinner(\"Building vector store...\"):\n",
        "        # Create vector store\n",
        "        vectorstore = FAISS.from_documents(\n",
        "            documents=chunks,\n",
        "            embedding=embeddings\n",
        "        )\n",
        "        st.success(\"âœ“ Vector store created\")\n",
        "\n",
        "    with st.spinner(\"Loading language model (this may take a few minutes)...\"):\n",
        "        # Initialize LLM pipeline with GPU and optimizations\n",
        "\n",
        "        pipe = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "            device=0 if device == \"cuda\" else -1,  # 0 = GPU, -1 = CPU\n",
        "            torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        "            temperature=0.001,\n",
        "            max_new_tokens=200,\n",
        "            return_full_text=False\n",
        "        )\n",
        "        llm = HuggingFacePipeline(pipeline=pipe)\n",
        "        st.success(f\"âœ“ Language model loaded on {device}\")\n",
        "\n",
        "    # Create retriever\n",
        "    retriever = vectorstore.as_retriever(\n",
        "        search_type=\"similarity\",\n",
        "        search_kwargs={\"k\": 4}\n",
        "    )\n",
        "\n",
        "    # Define prompt template\n",
        "    template = \"\"\"\n",
        "You are a helpful assistant that answers using only the provided context.\n",
        "If the answer is not contained in the context, say \"I do not know\".\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Other requirement: Structure the answer in a way that is easy to read and understand.\n",
        "\"\"\"\n",
        "\n",
        "    prompt = PromptTemplate(\n",
        "        input_variables=[\"context\", \"question\"],\n",
        "        template=template\n",
        "    )\n",
        "\n",
        "    # Create chain\n",
        "    chain = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        retriever=retriever,\n",
        "        chain_type_kwargs={\"prompt\": prompt},\n",
        "        return_source_documents=True\n",
        "    )\n",
        "\n",
        "    return chain, retriever\n",
        "\n",
        "# Main app\n",
        "def main():\n",
        "    st.title(\"ðŸŽ“ Student Project RAG Chatbot\")\n",
        "    st.markdown(\"Ask questions about student projects, mentors, and evaluation criteria.\")\n",
        "\n",
        "    # Initialize chain (cached)\n",
        "    try:\n",
        "        chain, retriever = initialize_rag_chain()\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error initializing RAG system: {e}\")\n",
        "        st.stop()\n",
        "\n",
        "    # Sidebar\n",
        "    with st.sidebar:\n",
        "        st.header(\"Settings\")\n",
        "        show_sources = st.checkbox(\"Show source documents\", value=True)\n",
        "        st.markdown(\"---\")\n",
        "        st.markdown(\"### Example Questions\")\n",
        "        st.markdown(\"\"\"\n",
        "        - Who are the mentors for robotics projects?\n",
        "        - Which students worked on AI healthcare projects?\n",
        "        - What are the evaluation criteria?\n",
        "        - Provide names of students for a Robot and Air Quality project\n",
        "        \"\"\")\n",
        "\n",
        "    # Chat interface\n",
        "    if \"messages\" not in st.session_state:\n",
        "        st.session_state.messages = []\n",
        "\n",
        "    # Display chat history\n",
        "    for message in st.session_state.messages:\n",
        "        with st.chat_message(message[\"role\"]):\n",
        "            st.markdown(message[\"content\"])\n",
        "            if \"sources\" in message and show_sources:\n",
        "                with st.expander(\"ðŸ“š View Sources\"):\n",
        "                    st.markdown(message[\"sources\"])\n",
        "\n",
        "    # Chat input\n",
        "    if question := st.chat_input(\"Ask a question about student projects...\"):\n",
        "        # Add user message\n",
        "        st.session_state.messages.append({\"role\": \"user\", \"content\": question})\n",
        "        with st.chat_message(\"user\"):\n",
        "            st.markdown(question)\n",
        "\n",
        "        # Get response\n",
        "        with st.chat_message(\"assistant\"):\n",
        "            with st.spinner(\"Thinking...\"):\n",
        "                try:\n",
        "                    result = chain(question)\n",
        "                    answer = result[\"result\"]\n",
        "\n",
        "                    # Display answer\n",
        "                    st.markdown(answer)\n",
        "\n",
        "                    # Format sources\n",
        "                    sources_text = \"\"\n",
        "                    if show_sources and \"source_documents\" in result:\n",
        "                        sources_text = \"\\n\\n**Retrieved Sources:**\\n\\n\"\n",
        "                        for i, doc in enumerate(result[\"source_documents\"], 1):\n",
        "                            source = doc.metadata.get('source', 'Unknown')\n",
        "                            category = doc.metadata.get('category', 'N/A')\n",
        "                            sources_text += f\"{i}. **{source}** ({category})\\n\"\n",
        "                            sources_text += f\"   {doc.page_content[:200]}...\\n\\n\"\n",
        "\n",
        "                        with st.expander(\"ðŸ“š View Sources\"):\n",
        "                            st.markdown(sources_text)\n",
        "\n",
        "                    # Add to chat history\n",
        "                    st.session_state.messages.append({\n",
        "                        \"role\": \"assistant\",\n",
        "                        \"content\": answer,\n",
        "                        \"sources\": sources_text\n",
        "                    })\n",
        "\n",
        "                except Exception as e:\n",
        "                    error_msg = f\"Error generating response: {e}\"\n",
        "                    st.error(error_msg)\n",
        "                    st.session_state.messages.append({\n",
        "                        \"role\": \"assistant\",\n",
        "                        \"content\": error_msg\n",
        "                    })\n",
        "\n",
        "    # Clear chat button\n",
        "    if st.sidebar.button(\"ðŸ—‘ï¸ Clear Chat History\"):\n",
        "        st.session_state.messages = []\n",
        "        st.rerun()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "5_AWoF4lAtUI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15d5193b-a66e-4d5c-a663-020941ab7041"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run the app with ngrok"
      ],
      "metadata": {
        "id": "UI7UkiWuL0SO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "# Kill any existing streamlit processes\n",
        "!pkill -f streamlit\n",
        "\n",
        "# Start Streamlit in background\n",
        "streamlit_process = subprocess.Popen(\n",
        "    [\"streamlit\", \"run\", \"/content/app.py\", \"--server.port=8501\"],\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.PIPE\n",
        ")\n",
        "\n",
        "# Wait for Streamlit to start\n",
        "time.sleep(10)\n",
        "\n",
        "# Create ngrok tunnel\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"=\" * 50)\n",
        "print(f\"Streamlit app is running!\")\n",
        "print(f\"Public URL: {public_url}\")\n",
        "print(\"=\" * 50)\n",
        "print(\"\\nClick the URL above to access your app\")\n",
        "print(\"Keep this cell running to maintain the connection\")"
      ],
      "metadata": {
        "id": "fX1PYcWIA5w5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff1bda99-8959-4244-dc4e-7357fd6f52df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "Streamlit app is running!\n",
            "Public URL: NgrokTunnel: \"https://unmustered-lacresha-tetratomic.ngrok-free.dev\" -> \"http://localhost:8501\"\n",
            "==================================================\n",
            "\n",
            "Click the URL above to access your app\n",
            "Keep this cell running to maintain the connection\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nFJ-W97hBB4L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}