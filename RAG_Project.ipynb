{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "authorship_tag": "ABX9TyNWKG6BKbed56HPQ4zJ3gIC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c6ab26d1a39649a795c3dc3023d3509a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_350b482b4f4d4df5a0f09f87f5e34025",
              "IPY_MODEL_203bc5f72eb341deb00e211586bc082f",
              "IPY_MODEL_35fe7b42ca73493793ca21939fab497b"
            ],
            "layout": "IPY_MODEL_a6470f6557c44af3bce70b77b984809b"
          }
        },
        "350b482b4f4d4df5a0f09f87f5e34025": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b3406758b51438aaca3054d121a3df5",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_be6c43306ee24cf5ae1017bd20d16cf5",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "203bc5f72eb341deb00e211586bc082f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a87e8267bdb74023b8b66559ba63f20b",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aa9b8fe5eb9e434183e184fc347cf546",
            "value": 3
          }
        },
        "35fe7b42ca73493793ca21939fab497b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_601d64d4defd4d998b5fe8ebf13c0fe1",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_1cd2ef8b31024cbab31db6b0ca7b392b",
            "value": "‚Äá3/3‚Äá[00:04&lt;00:00,‚Äá‚Äá1.40s/it]"
          }
        },
        "a6470f6557c44af3bce70b77b984809b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b3406758b51438aaca3054d121a3df5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be6c43306ee24cf5ae1017bd20d16cf5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a87e8267bdb74023b8b66559ba63f20b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa9b8fe5eb9e434183e184fc347cf546": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "601d64d4defd4d998b5fe8ebf13c0fe1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1cd2ef8b31024cbab31db6b0ca7b392b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Spykabore15/students_RAG_project/blob/main/RAG_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mount Drive and Install Dependencies"
      ],
      "metadata": {
        "id": "UFSS07eYLleY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbWzdgjzAesk",
        "outputId": "89e02d3d-71ee-4b6a-a2b5-38eab2428232"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "‚úÖ All dependencies installed!\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install -q \\\n",
        "  langchain \\\n",
        "  langchain-community==0.2.* \\\n",
        "  faiss-cpu \\\n",
        "  sentence-transformers \\\n",
        "  transformers \\\n",
        "  accelerate \\\n",
        "  pypdf==4.* \\\n",
        "  streamlit \\\n",
        "  pyngrok\n",
        "\n",
        "print(\"‚úÖ All dependencies installed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set up ngrok"
      ],
      "metadata": {
        "id": "4oqa7JPlLhf9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up ngrok\n",
        "from pyngrok import ngrok\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Get ngrok token from secrets\n",
        "ngrok_token = userdata.get('NGROK_AUTH_KEY')\n",
        "ngrok.set_auth_token(ngrok_token)\n"
      ],
      "metadata": {
        "id": "I8Ea-O3sApkw"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Streamit app configuration"
      ],
      "metadata": {
        "id": "ij6yy_CHLqlD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/app.py\n",
        "import streamlit as st\n",
        "from pathlib import Path\n",
        "import json\n",
        "from typing import List, Tuple\n",
        "import torch\n",
        "\n",
        "# Langchain imports\n",
        "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain import HuggingFacePipeline\n",
        "from langchain.prompts import PromptTemplate\n",
        "from transformers import pipeline\n",
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "# Page config\n",
        "st.set_page_config(\n",
        "    page_title=\"Enhanced Student RAG Chatbot\",\n",
        "    page_icon=\"üéì\",\n",
        "    layout=\"wide\"\n",
        ")\n",
        "\n",
        "# Constants\n",
        "DATA_PATH = \"/content/drive/MyDrive/TRAINING DATA\"\n",
        "\n",
        "# ==================== QUERY DECOMPOSITION ====================\n",
        "\n",
        "DECOMPOSITION_PROMPT = \"\"\"You are a query analyzer. Analyze if this question needs to be broken down into sub-questions.\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Rules:\n",
        "1. If the question is simple and focused, respond with: SIMPLE\n",
        "2. If the question has multiple parts or requires information from different sources, decompose it into 2-4 sub-questions.\n",
        "\n",
        "Response format for complex questions:\n",
        "SUB-QUERY 1: [first sub-question]\n",
        "SUB-QUERY 2: [second sub-question]\n",
        "SUB-QUERY 3: [third sub-question] (if needed)\n",
        "\n",
        "Examples:\n",
        "\n",
        "Question: \"Who is Dr. Sarah Chen?\"\n",
        "Response: SIMPLE\n",
        "\n",
        "Question: \"Which students worked with Dr. Sarah Chen on AI healthcare projects and what were their evaluation scores?\"\n",
        "Response:\n",
        "SUB-QUERY 1: Which students worked with Dr. Sarah Chen?\n",
        "SUB-QUERY 2: What AI healthcare projects exist in the database?\n",
        "SUB-QUERY 3: What were the evaluation scores for these students?\n",
        "\n",
        "Question: \"Compare robotics projects with environmental science projects\"\n",
        "Response:\n",
        "SUB-QUERY 1: What robotics projects are in the database?\n",
        "SUB-QUERY 2: What environmental science projects are in the database?\n",
        "\n",
        "Now analyze this question:\n",
        "Question: {question}\n",
        "Response:\"\"\"\n",
        "\n",
        "def decompose_query(llm, question: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Use LLM to decide if decomposition is needed\n",
        "    Returns: list of queries (original or decomposed)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        prompt = DECOMPOSITION_PROMPT.format(question=question)\n",
        "        response = llm.predict(prompt)\n",
        "\n",
        "        # Parse response\n",
        "        if \"SIMPLE\" in response:\n",
        "            return [question]\n",
        "\n",
        "        # Extract sub-queries\n",
        "        sub_queries = []\n",
        "        for line in response.split('\\n'):\n",
        "            if line.strip().startswith('SUB-QUERY'):\n",
        "                # Extract query after the colon\n",
        "                query = line.split(':', 1)[1].strip()\n",
        "                if query:\n",
        "                    sub_queries.append(query)\n",
        "\n",
        "        # Fallback: if parsing failed, use original\n",
        "        return sub_queries if sub_queries else [question]\n",
        "    except Exception as e:\n",
        "        st.warning(f\"Decomposition failed: {e}. Using original query.\")\n",
        "        return [question]\n",
        "\n",
        "# ==================== RERANKING ====================\n",
        "\n",
        "class CrossEncoderReranker:\n",
        "    \"\"\"Wrapper for cross-encoder reranking\"\"\"\n",
        "\n",
        "    def __init__(self, model_name=\"cross-encoder/ms-marco-MiniLM-L-6-v2\", device=\"cuda\"):\n",
        "        self.device = device\n",
        "        self.model = CrossEncoder(model_name, device=device)\n",
        "\n",
        "    def rerank(self, query: str, documents: List[Document], top_k: int = 4) -> List[Tuple[Document, float]]:\n",
        "        \"\"\"\n",
        "        Rerank documents based on relevance to query\n",
        "\n",
        "        Args:\n",
        "            query: User question\n",
        "            documents: List of Document objects\n",
        "            top_k: Number of top documents to return\n",
        "\n",
        "        Returns:\n",
        "            List of (document, score) tuples, sorted by score\n",
        "        \"\"\"\n",
        "        if not documents:\n",
        "            return []\n",
        "\n",
        "        # Prepare pairs for scoring\n",
        "        pairs = [[query, doc.page_content] for doc in documents]\n",
        "\n",
        "        # Get relevance scores\n",
        "        scores = self.model.predict(pairs)\n",
        "\n",
        "        # Combine documents with scores\n",
        "        doc_scores = list(zip(documents, scores))\n",
        "\n",
        "        # Sort by score (descending)\n",
        "        doc_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        return doc_scores[:top_k]\n",
        "\n",
        "# ==================== ENHANCED RAG PIPELINE ====================\n",
        "\n",
        "class EnhancedRAGPipeline:\n",
        "    \"\"\"Complete RAG pipeline with decomposition and reranking\"\"\"\n",
        "\n",
        "    def __init__(self, vectorstore, llm, reranker, retrieval_k=20, final_k=4):\n",
        "        self.vectorstore = vectorstore\n",
        "        self.llm = llm\n",
        "        self.reranker = reranker\n",
        "        self.retrieval_k = retrieval_k\n",
        "        self.final_k = final_k\n",
        "\n",
        "        # Retriever with higher k for reranking\n",
        "        self.retriever = vectorstore.as_retriever(\n",
        "            search_kwargs={\"k\": retrieval_k}\n",
        "        )\n",
        "\n",
        "    def retrieve_documents(self, sub_queries: List[str]) -> List[Document]:\n",
        "        \"\"\"\n",
        "        Retrieve documents for all sub-queries and deduplicate\n",
        "        \"\"\"\n",
        "        all_docs = []\n",
        "\n",
        "        # Retrieve for each sub-query\n",
        "        for sub_q in sub_queries:\n",
        "            docs = self.retriever.get_relevant_documents(sub_q)\n",
        "            all_docs.extend(docs)\n",
        "\n",
        "        # Deduplicate by content hash\n",
        "        unique_docs = []\n",
        "        seen_content = set()\n",
        "        for doc in all_docs:\n",
        "            content_hash = hash(doc.page_content)\n",
        "            if content_hash not in seen_content:\n",
        "                unique_docs.append(doc)\n",
        "                seen_content.add(content_hash)\n",
        "\n",
        "        return unique_docs\n",
        "\n",
        "    def generate_answer(self, question: str, context_docs: List[Tuple[Document, float]]) -> str:\n",
        "        \"\"\"\n",
        "        Generate final answer using LLM\n",
        "        \"\"\"\n",
        "        # Format context with sources\n",
        "        context = \"\\n\\n\".join([\n",
        "            f\"[Source {i+1} - Relevance: {score:.2f}]\\n{doc.page_content}\"\n",
        "            for i, (doc, score) in enumerate(context_docs)\n",
        "        ])\n",
        "\n",
        "        prompt = f\"\"\"You are a helpful assistant that answers using only the provided context.\n",
        "If the answer is not contained in the context, say \"I do not know\".\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer (be concise, structured, and cite sources when relevant):\"\"\"\n",
        "\n",
        "        answer = self.llm.predict(prompt)\n",
        "        return answer\n",
        "\n",
        "    def query(self, question: str, enable_decomposition: bool = True, enable_reranking: bool = True):\n",
        "        \"\"\"\n",
        "        Full RAG pipeline with all enhancements\n",
        "\n",
        "        Returns:\n",
        "            dict with answer, sub_queries, sources, and metadata\n",
        "        \"\"\"\n",
        "        # Step 1: Query Decomposition\n",
        "        if enable_decomposition:\n",
        "            sub_queries = decompose_query(self.llm, question)\n",
        "        else:\n",
        "            sub_queries = [question]\n",
        "\n",
        "        # Step 2: Retrieve documents\n",
        "        retrieved_docs = self.retrieve_documents(sub_queries)\n",
        "\n",
        "        # Step 3: Rerank documents\n",
        "        if enable_reranking and retrieved_docs:\n",
        "            # Use original question for reranking (not sub-queries)\n",
        "            reranked_docs = self.reranker.rerank(\n",
        "                query=question,\n",
        "                documents=retrieved_docs,\n",
        "                top_k=self.final_k\n",
        "            )\n",
        "        else:\n",
        "            # No reranking, just take top k\n",
        "            reranked_docs = [(doc, 1.0) for doc in retrieved_docs[:self.final_k]]\n",
        "\n",
        "        # Step 4: Generate answer\n",
        "        answer = self.generate_answer(question, reranked_docs)\n",
        "\n",
        "        return {\n",
        "            \"answer\": answer,\n",
        "            \"sub_queries\": sub_queries,\n",
        "            \"sources\": reranked_docs,\n",
        "            \"num_retrieved\": len(retrieved_docs),\n",
        "            \"num_sources\": len(reranked_docs)\n",
        "        }\n",
        "\n",
        "# ==================== DOCUMENT LOADING (CACHED) ====================\n",
        "\n",
        "@st.cache_resource\n",
        "def load_core_pdfs(data_path: Path) -> List[Document]:\n",
        "    \"\"\"Load core PDF documents with caching\"\"\"\n",
        "    core_pdfs = [\n",
        "        data_path / \"Projects.pdf\",\n",
        "        data_path / \"criteria.pdf\",\n",
        "        data_path / \"Mentors.pdf\"\n",
        "    ]\n",
        "\n",
        "    docs = []\n",
        "    for pdf in core_pdfs:\n",
        "        if pdf.exists():\n",
        "            pages = PyPDFLoader(str(pdf)).load()\n",
        "            for p in pages:\n",
        "                p.metadata.update({\"source\": pdf.name, \"category\": \"core\"})\n",
        "            docs.extend(pages)\n",
        "    return docs\n",
        "\n",
        "@st.cache_resource\n",
        "def load_student_dirs(data_path: Path) -> List[Document]:\n",
        "    \"\"\"Load student directories with caching\"\"\"\n",
        "    docs = []\n",
        "    students_dir = data_path / \"students\"\n",
        "\n",
        "    if not students_dir.exists():\n",
        "        return docs\n",
        "\n",
        "    for student_dir in students_dir.iterdir():\n",
        "        if not student_dir.is_dir():\n",
        "            continue\n",
        "\n",
        "        # Load student report\n",
        "        report_pdf = student_dir / \"report.pdf\"\n",
        "        if report_pdf.exists():\n",
        "            pages = PyPDFLoader(str(report_pdf)).load()\n",
        "            for p in pages:\n",
        "                p.metadata.update({\n",
        "                    \"source\": f\"{student_dir.name}/report.pdf\",\n",
        "                    \"student\": student_dir.name,\n",
        "                    \"category\": \"student_report\"\n",
        "                })\n",
        "            docs.extend(pages)\n",
        "\n",
        "        # Load student summary\n",
        "        summary_txt = student_dir / \"summary.txt\"\n",
        "        if summary_txt.exists():\n",
        "            tdocs = TextLoader(str(summary_txt), encoding=\"utf-8\").load()\n",
        "            for d in tdocs:\n",
        "                d.metadata.update({\n",
        "                    \"source\": f\"{student_dir.name}/summary.txt\",\n",
        "                    \"student\": student_dir.name,\n",
        "                    \"category\": \"student_summary\"\n",
        "                })\n",
        "            docs.extend(tdocs)\n",
        "\n",
        "        # Load student metadata\n",
        "        meta_json = student_dir / \"metadata.json\"\n",
        "        if meta_json.exists():\n",
        "            try:\n",
        "                meta = json.loads(meta_json.read_text(encoding=\"utf-8\"))\n",
        "                meta_doc = Document(\n",
        "                    page_content=json.dumps(meta, ensure_ascii=False, indent=2),\n",
        "                    metadata={\n",
        "                        \"source\": f\"{student_dir.name}/metadata.json\",\n",
        "                        \"student\": student_dir.name,\n",
        "                        \"category\": \"student_metadata\"\n",
        "                    }\n",
        "                )\n",
        "                docs.append(meta_doc)\n",
        "            except Exception as e:\n",
        "                st.warning(f\"Couldn't parse {meta_json}: {e}\")\n",
        "\n",
        "    return docs\n",
        "\n",
        "# ==================== MAIN INITIALIZATION (CACHED) ====================\n",
        "\n",
        "@st.cache_resource\n",
        "def initialize_enhanced_rag():\n",
        "    \"\"\"Initialize the complete enhanced RAG system with caching\"\"\"\n",
        "\n",
        "    # Check GPU availability\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    st.info(f\"üñ•Ô∏è Using device: **{device}**\")\n",
        "\n",
        "    if device == \"cpu\":\n",
        "        st.warning(\"‚ö†Ô∏è GPU not detected! This will be slower. Enable GPU in Colab Runtime settings.\")\n",
        "\n",
        "    with st.spinner(\"üìö Loading documents...\"):\n",
        "        data_path = Path(DATA_PATH)\n",
        "        raw_docs = load_core_pdfs(data_path) + load_student_dirs(data_path)\n",
        "        st.success(f\"‚úÖ Loaded **{len(raw_docs)}** documents\")\n",
        "\n",
        "    with st.spinner(\" Splitting documents into chunks...\"):\n",
        "        splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=1000,\n",
        "            chunk_overlap=150,\n",
        "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        "        )\n",
        "        chunks = splitter.split_documents(raw_docs)\n",
        "        st.success(f\"‚úÖ Created **{len(chunks)}** chunks\")\n",
        "\n",
        "    with st.spinner(\"Loading embedding model...\"):\n",
        "        embeddings = HuggingFaceEmbeddings(\n",
        "            model_name=\"all-MiniLM-L6-v2\",\n",
        "            model_kwargs={'device': device}\n",
        "        )\n",
        "        st.success(\"‚úÖ Embeddings model loaded\")\n",
        "\n",
        "    with st.spinner(\" Building vector store...\"):\n",
        "        vectorstore = FAISS.from_documents(\n",
        "            documents=chunks,\n",
        "            embedding=embeddings\n",
        "        )\n",
        "        st.success(\"‚úÖ Vector store created\")\n",
        "\n",
        "    with st.spinner(\"ü§ñ Loading language model (Mistral-7B)...\"):\n",
        "        pipe = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "            device=0 if device == \"cuda\" else -1,\n",
        "            torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        "            max_new_tokens=300,\n",
        "            temperature=0.1,\n",
        "            return_full_text=False\n",
        "        )\n",
        "        llm = HuggingFacePipeline(pipeline=pipe)\n",
        "        st.success(f\"‚úÖ Language model loaded on **{device}**\")\n",
        "\n",
        "    with st.spinner(\"üéØ Loading reranker (Cross-Encoder)...\"):\n",
        "        reranker = CrossEncoderReranker(device=device)\n",
        "        st.success(\"‚úÖ Reranker loaded\")\n",
        "\n",
        "    # Create enhanced RAG pipeline\n",
        "    rag = EnhancedRAGPipeline(\n",
        "        vectorstore=vectorstore,\n",
        "        llm=llm,\n",
        "        reranker=reranker,\n",
        "        retrieval_k=20,\n",
        "        final_k=4\n",
        "    )\n",
        "\n",
        "    st.success(\" **Enhanced RAG system initialized!**\")\n",
        "\n",
        "    return rag\n",
        "\n",
        "# ==================== STREAMLIT UI ====================\n",
        "\n",
        "def main():\n",
        "    st.title(\"üéì Enhanced Student Project RAG Chatbot\")\n",
        "    st.markdown(\"\"\"\n",
        "    Ask questions about student projects, mentors, and evaluation criteria.\n",
        "    **New Features:** Query decomposition + Advanced reranking\n",
        "    \"\"\")\n",
        "\n",
        "    # Sidebar configuration\n",
        "    with st.sidebar:\n",
        "        st.header(\"‚öôÔ∏è Configuration\")\n",
        "\n",
        "        enable_decomposition = st.checkbox(\n",
        "            \"Enable Query Decomposition\",\n",
        "            value=True,\n",
        "            help=\"Break complex questions into simpler sub-queries\"\n",
        "        )\n",
        "\n",
        "        enable_reranking = st.checkbox(\n",
        "            \"Enable Reranking\",\n",
        "            value=True,\n",
        "            help=\"Reorder retrieved documents by relevance\"\n",
        "        )\n",
        "\n",
        "        show_details = st.checkbox(\n",
        "            \"Show Processing Details\",\n",
        "            value=True,\n",
        "            help=\"Display sub-queries and relevance scores\"\n",
        "        )\n",
        "\n",
        "        st.markdown(\"---\")\n",
        "        st.markdown(\"### üí° Example Questions\")\n",
        "        st.markdown(\"\"\"\n",
        "        **Simple:**\n",
        "        - Who are the mentors for robotics projects?\n",
        "        - What is Dr. Sarah Chen's expertise?\n",
        "\n",
        "        **Complex (will decompose):**\n",
        "        - Which students worked with Dr. Chen on AI healthcare projects and what were their scores?\n",
        "        - Compare robotics projects with environmental science projects\n",
        "        - List students who can collaborate on Robot and Air Quality projects\n",
        "        \"\"\")\n",
        "\n",
        "        st.markdown(\"---\")\n",
        "        st.markdown(\"### System Info\")\n",
        "        if torch.cuda.is_available():\n",
        "            st.success(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "        else:\n",
        "            st.error(\"‚ùå No GPU detected\")\n",
        "\n",
        "    # Initialize RAG system\n",
        "    try:\n",
        "        rag = initialize_enhanced_rag()\n",
        "    except Exception as e:\n",
        "        st.error(f\"‚ùå Error initializing RAG system: {e}\")\n",
        "        st.stop()\n",
        "\n",
        "    # Chat interface\n",
        "    if \"messages\" not in st.session_state:\n",
        "        st.session_state.messages = []\n",
        "\n",
        "    # Display chat history\n",
        "    for message in st.session_state.messages:\n",
        "        with st.chat_message(message[\"role\"]):\n",
        "            st.markdown(message[\"content\"])\n",
        "\n",
        "            # Show details if available\n",
        "            if \"details\" in message and show_details:\n",
        "                with st.expander(\"üîç Processing Details\"):\n",
        "                    st.markdown(message[\"details\"])\n",
        "\n",
        "            # Show sources if available\n",
        "            if \"sources\" in message:\n",
        "                with st.expander(\"üìö Source Documents\"):\n",
        "                    st.markdown(message[\"sources\"])\n",
        "\n",
        "    # Chat input\n",
        "    if question := st.chat_input(\"Ask a question about student projects...\"):\n",
        "        # Add user message\n",
        "        st.session_state.messages.append({\"role\": \"user\", \"content\": question})\n",
        "        with st.chat_message(\"user\"):\n",
        "            st.markdown(question)\n",
        "\n",
        "        # Get response\n",
        "        with st.chat_message(\"assistant\"):\n",
        "            with st.spinner(\"ü§î Processing your question...\"):\n",
        "                try:\n",
        "                    # Query the RAG system\n",
        "                    result = rag.query(\n",
        "                        question=question,\n",
        "                        enable_decomposition=enable_decomposition,\n",
        "                        enable_reranking=enable_reranking\n",
        "                    )\n",
        "\n",
        "                    # Display answer\n",
        "                    st.markdown(result[\"answer\"])\n",
        "\n",
        "                    # Format processing details\n",
        "                    details_text = \"\"\n",
        "                    if show_details:\n",
        "                        if len(result[\"sub_queries\"]) > 1:\n",
        "                            details_text += \"**üîç Query Decomposition:**\\n\"\n",
        "                            for i, sq in enumerate(result[\"sub_queries\"], 1):\n",
        "                                details_text += f\"{i}. {sq}\\n\"\n",
        "                            details_text += \"\\n\"\n",
        "\n",
        "                        details_text += f\"**Retrieval Stats:**\\n\"\n",
        "                        details_text += f\"- Retrieved: {result['num_retrieved']} documents\\n\"\n",
        "                        details_text += f\"- After reranking: {result['num_sources']} documents\\n\"\n",
        "\n",
        "                        with st.expander(\"üîç Processing Details\"):\n",
        "                            st.markdown(details_text)\n",
        "\n",
        "                    # Format sources\n",
        "                    sources_text = \"\"\n",
        "                    for i, (doc, score) in enumerate(result[\"sources\"], 1):\n",
        "                        source = doc.metadata.get('source', 'Unknown')\n",
        "                        category = doc.metadata.get('category', 'N/A')\n",
        "                        student = doc.metadata.get('student', 'N/A')\n",
        "\n",
        "                        sources_text += f\"**{i}. {source}** (Score: {score:.3f})\\n\"\n",
        "                        sources_text += f\"   - Category: {category}\\n\"\n",
        "                        if student != 'N/A':\n",
        "                            sources_text += f\"   - Student: {student}\\n\"\n",
        "                        sources_text += f\"   - Content preview: {doc.page_content[:200]}...\\n\\n\"\n",
        "\n",
        "                    with st.expander(\"üìö Source Documents\"):\n",
        "                        st.markdown(sources_text)\n",
        "\n",
        "                    # Add to chat history\n",
        "                    st.session_state.messages.append({\n",
        "                        \"role\": \"assistant\",\n",
        "                        \"content\": result[\"answer\"],\n",
        "                        \"details\": details_text if show_details else None,\n",
        "                        \"sources\": sources_text\n",
        "                    })\n",
        "\n",
        "                except Exception as e:\n",
        "                    error_msg = f\"‚ùå Error generating response: {str(e)}\"\n",
        "                    st.error(error_msg)\n",
        "                    st.session_state.messages.append({\n",
        "                        \"role\": \"assistant\",\n",
        "                        \"content\": error_msg\n",
        "                    })\n",
        "\n",
        "    # Action buttons in sidebar\n",
        "    with st.sidebar:\n",
        "        st.markdown(\"---\")\n",
        "        col1, col2 = st.columns(2)\n",
        "\n",
        "        with col1:\n",
        "            if st.button(\"üóëÔ∏è Clear Chat\", use_container_width=True):\n",
        "                st.session_state.messages = []\n",
        "                st.rerun()\n",
        "\n",
        "        with col2:\n",
        "            if st.button(\"üîÑ Reload System\", use_container_width=True):\n",
        "                st.cache_resource.clear()\n",
        "                st.rerun()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "5_AWoF4lAtUI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e575fcd3-e8ce-4996-8309-53fe87e75b89"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optional - Pre-build and Save Vector Store\n",
        "\n",
        "Run this once to save time"
      ],
      "metadata": {
        "id": "rhPcUHToJDfR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import torch\n",
        "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "import json\n",
        "\n",
        "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
        "\n",
        "DATA_PATH = \"/content/drive/MyDrive/TRAINING DATA\"\n",
        "VECTORSTORE_PATH = \"/content/drive/MyDrive/vectorstore_enhanced\"\n",
        "\n",
        "def load_all_documents():\n",
        "    \"\"\"Load all documents from the data directory\"\"\"\n",
        "    data_path = Path(DATA_PATH)\n",
        "    docs = []\n",
        "\n",
        "    # Load core PDFs\n",
        "    core_pdfs = [\n",
        "        data_path / \"Projects.pdf\",\n",
        "        data_path / \"criteria.pdf\",\n",
        "        data_path / \"Mentors.pdf\"\n",
        "    ]\n",
        "\n",
        "    for pdf in core_pdfs:\n",
        "        if pdf.exists():\n",
        "            pages = PyPDFLoader(str(pdf)).load()\n",
        "            for p in pages:\n",
        "                p.metadata.update({\"source\": pdf.name, \"category\": \"core\"})\n",
        "            docs.extend(pages)\n",
        "\n",
        "    # Load student directories\n",
        "    students_dir = data_path / \"students\"\n",
        "    if students_dir.exists():\n",
        "        for student_dir in students_dir.iterdir():\n",
        "            if not student_dir.is_dir():\n",
        "                continue\n",
        "\n",
        "            # Report PDF\n",
        "            report_pdf = student_dir / \"report.pdf\"\n",
        "            if report_pdf.exists():\n",
        "                pages = PyPDFLoader(str(report_pdf)).load()\n",
        "                for p in pages:\n",
        "                    p.metadata.update({\n",
        "                        \"source\": f\"{student_dir.name}/report.pdf\",\n",
        "                        \"student\": student_dir.name,\n",
        "                        \"category\": \"student_report\"\n",
        "                    })\n",
        "                docs.extend(pages)\n",
        "\n",
        "            # Summary TXT\n",
        "            summary_txt = student_dir / \"summary.txt\"\n",
        "            if summary_txt.exists():\n",
        "                tdocs = TextLoader(str(summary_txt), encoding=\"utf-8\").load()\n",
        "                for d in tdocs:\n",
        "                    d.metadata.update({\n",
        "                        \"source\": f\"{student_dir.name}/summary.txt\",\n",
        "                        \"student\": student_dir.name,\n",
        "                        \"category\": \"student_summary\"\n",
        "                    })\n",
        "                docs.extend(tdocs)\n",
        "\n",
        "            # Metadata JSON\n",
        "            meta_json = student_dir / \"metadata.json\"\n",
        "            if meta_json.exists():\n",
        "                try:\n",
        "                    meta = json.loads(meta_json.read_text(encoding=\"utf-8\"))\n",
        "                    meta_doc = Document(\n",
        "                        page_content=json.dumps(meta, ensure_ascii=False, indent=2),\n",
        "                        metadata={\n",
        "                            \"source\": f\"{student_dir.name}/metadata.json\",\n",
        "                            \"student\": student_dir.name,\n",
        "                            \"category\": \"student_metadata\"\n",
        "                        }\n",
        "                    )\n",
        "                    docs.append(meta_doc)\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ö†Ô∏è Couldn't parse {meta_json}: {e}\")\n",
        "\n",
        "    return docs\n",
        "\n",
        "# Load documents\n",
        "print(\"üìö Loading documents...\")\n",
        "raw_docs = load_all_documents()\n",
        "print(f\"‚úÖ Loaded {len(raw_docs)} documents\")\n",
        "\n",
        "# Split into chunks\n",
        "print(\"Splitting documents...\")\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=150,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        ")\n",
        "chunks = splitter.split_documents(raw_docs)\n",
        "print(f\"‚úÖ Created {len(chunks)} chunks\")\n",
        "\n",
        "# Create embeddings\n",
        "print(\" Creating embeddings...\")\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"all-MiniLM-L6-v2\",\n",
        "    model_kwargs={'device': device}\n",
        ")\n",
        "print(f\"‚úÖ Embeddings model loaded on {device}\")\n",
        "\n",
        "# Build vector store\n",
        "print(\"Building vector store (this may take 2-3 minutes)...\")\n",
        "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
        "print(\"‚úÖ Vector store created!\")\n",
        "\n",
        "# Save to disk\n",
        "print(f\" Saving vector store to {VECTORSTORE_PATH}...\")\n",
        "vectorstore.save_local(VECTORSTORE_PATH)\n",
        "print(\"‚úÖ Vector store saved! You can now skip this cell in future runs.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwjkCIutHWW1",
        "outputId": "ad3bf604-f7a5-4bdc-990e-8cdd2f661778"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Available: True\n",
            "üìö Loading documents...\n",
            "‚úÖ Loaded 134 documents\n",
            "Splitting documents...\n",
            "‚úÖ Created 287 chunks\n",
            " Creating embeddings...\n",
            "‚úÖ Embeddings model loaded on cuda\n",
            "Building vector store (this may take 2-3 minutes)...\n",
            "‚úÖ Vector store created!\n",
            " Saving vector store to /content/drive/MyDrive/vectorstore_enhanced...\n",
            "‚úÖ Vector store saved! You can now skip this cell in future runs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run the app with ngrok"
      ],
      "metadata": {
        "id": "UI7UkiWuL0SO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import time\n",
        "import requests\n",
        "\n",
        "# Kill any existing streamlit processes\n",
        "!pkill -f streamlit\n",
        "\n",
        "# Wait a moment for cleanup\n",
        "time.sleep(2)\n",
        "\n",
        "# Start Streamlit in background\n",
        "print(\"Starting Streamlit...\")\n",
        "streamlit_process = subprocess.Popen(\n",
        "    [\"streamlit\", \"run\", \"/content/app.py\",\n",
        "     \"--server.port=8501\",\n",
        "     \"--server.address=localhost\",  # Changed from 0.0.0.0\n",
        "     \"--server.headless=true\"],\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.PIPE\n",
        ")\n",
        "\n",
        "# Wait for Streamlit to actually start (checking if it's ready)\n",
        "print(\"‚è≥ Waiting for Streamlit to initialize...\")\n",
        "max_retries = 5\n",
        "for i in range(max_retries):\n",
        "    try:\n",
        "        response = requests.get(\"http://localhost:8501\")\n",
        "        if response.status_code == 200:\n",
        "            print(\"‚úÖ Streamlit is ready!\")\n",
        "            break\n",
        "    except:\n",
        "        pass\n",
        "    time.sleep(2)\n",
        "    print(f\"   Attempt {i+1}/{max_retries}...\")\n",
        "else:\n",
        "    print(\"‚ùå Streamlit failed to start. Check logs below:\")\n",
        "    print(streamlit_process.stderr.read().decode())\n",
        "    raise Exception(\"Streamlit didn't start properly\")\n",
        "\n",
        "# Now create ngrok tunnel\n",
        "print(\" Creating ngrok tunnel...\")\n",
        "public_url = ngrok.connect(8501)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\" Enhanced RAG Chatbot is LIVE!\")\n",
        "print(\"=\" * 60)\n",
        "print(f\" Public URL: {public_url}\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\n‚úÖ Features Enabled:\")\n",
        "print(\"  - Query Decomposition (LLM-based)\")\n",
        "print(\"  - Cross-Encoder Reranking\")\n",
        "print(\"  - Enhanced Retrieval (k=20 ‚Üí top 4)\")\n",
        "print(\"  - Source Attribution with Scores\")\n",
        "print(\"\\n‚ö†Ô∏è Keep this cell running to maintain the connection\")\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "id": "fX1PYcWIA5w5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb6dfa02-0bad-45f2-e9cd-a8a776222b30"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Streamlit...\n",
            "‚è≥ Waiting for Streamlit to initialize...\n",
            "   Attempt 1/5...\n",
            "‚úÖ Streamlit is ready!\n",
            " Creating ngrok tunnel...\n",
            "============================================================\n",
            " Enhanced RAG Chatbot is LIVE!\n",
            "============================================================\n",
            " Public URL: NgrokTunnel: \"https://unmustered-lacresha-tetratomic.ngrok-free.dev\" -> \"http://localhost:8501\"\n",
            "============================================================\n",
            "\n",
            "‚úÖ Features Enabled:\n",
            "  - Query Decomposition (LLM-based)\n",
            "  - Cross-Encoder Reranking\n",
            "  - Enhanced Retrieval (k=20 ‚Üí top 4)\n",
            "  - Source Attribution with Scores\n",
            "\n",
            "‚ö†Ô∏è Keep this cell running to maintain the connection\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optional - Test the System Directly in Notebook"
      ],
      "metadata": {
        "id": "Ik8XVK8iOjRC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test individual components before running Streamlit\n",
        "\n",
        "from transformers import pipeline\n",
        "from langchain import HuggingFacePipeline\n",
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "# Load models\n",
        "print(\"Loading models for testing...\")\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "    device=device,\n",
        "    torch_dtype=torch.float16 if device >= 0 else torch.float32,\n",
        "    max_new_tokens=100,\n",
        "    return_full_text=False\n",
        ")\n",
        "llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "# Test decomposition\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Testing Query Decomposition\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "test_questions = [\n",
        "    \"Who is Dr. Sarah Chen?\",\n",
        "    \"Which students worked with Dr. Chen and what were their scores?\",\n",
        "    \"Compare robotics and environmental projects\"\n",
        "]\n",
        "\n",
        "DECOMP_PROMPT = \"\"\"You are a query analyzer. Analyze if this question needs to be broken down.\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Rules:\n",
        "1. If simple, respond with: SIMPLE\n",
        "2. If complex, decompose into sub-questions.\n",
        "\n",
        "Response format:\n",
        "SUB-QUERY 1: [question]\n",
        "SUB-QUERY 2: [question]\n",
        "\n",
        "Now analyze:\n",
        "Question: {question}\n",
        "Response:\"\"\"\n",
        "\n",
        "for q in test_questions:\n",
        "    prompt = DECOMP_PROMPT.format(question=q)\n",
        "    response = llm.predict(prompt)\n",
        "    print(f\"\\nQ: {q}\")\n",
        "    print(f\"Response: {response[:200]}\")\n",
        "\n",
        "# Test reranker\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Testing Cross-Encoder Reranker\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
        "\n",
        "query = \"AI healthcare projects\"\n",
        "test_docs = [\n",
        "    \"This project focuses on AI-powered medical diagnosis using deep learning.\",\n",
        "    \"Students worked on environmental monitoring systems.\",\n",
        "    \"The healthcare AI system achieved 95% accuracy in disease prediction.\",\n",
        "    \"Robotics project for autonomous delivery in hospitals.\"\n",
        "]\n",
        "\n",
        "pairs = [[query, doc] for doc in test_docs]\n",
        "scores = reranker.predict(pairs)\n",
        "\n",
        "print(f\"Query: {query}\\n\")\n",
        "for doc, score in sorted(zip(test_docs, scores), key=lambda x: x[1], reverse=True):\n",
        "    print(f\"Score: {score:.3f} - {doc[:60]}...\")\n",
        "\n",
        "print(\"\\n‚úÖ All components working! Ready to use Streamlit app.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495,
          "referenced_widgets": [
            "c6ab26d1a39649a795c3dc3023d3509a",
            "350b482b4f4d4df5a0f09f87f5e34025",
            "203bc5f72eb341deb00e211586bc082f",
            "35fe7b42ca73493793ca21939fab497b",
            "a6470f6557c44af3bce70b77b984809b",
            "2b3406758b51438aaca3054d121a3df5",
            "be6c43306ee24cf5ae1017bd20d16cf5",
            "a87e8267bdb74023b8b66559ba63f20b",
            "aa9b8fe5eb9e434183e184fc347cf546",
            "601d64d4defd4d998b5fe8ebf13c0fe1",
            "1cd2ef8b31024cbab31db6b0ca7b392b"
          ]
        },
        "id": "HSglcyJRH_Yp",
        "outputId": "8e8784ab-51a0-4674-fb1d-8e50bbf92e10"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading models for testing...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c6ab26d1a39649a795c3dc3023d3509a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 3.38 MiB is free. Process 132218 has 22.15 GiB memory in use. Of the allocated memory 21.76 GiB is allocated by PyTorch, and 169.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1269056518.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m pipe = pipeline(\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;34m\"text-generation\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mistralai/Mistral-7B-Instruct-v0.2\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/__init__.py\u001b[0m in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m   1227\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"processor\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1229\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpipeline_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframework\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         self.check_model_type(\n\u001b[1;32m    123\u001b[0m             \u001b[0mTF_MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"tf\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mMODEL_FOR_CAUSAL_LM_MAPPING_NAMES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, tokenizer, feature_extractor, image_processor, processor, modelcard, framework, task, device, binary_output, **kwargs)\u001b[0m\n\u001b[1;32m   1042\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mhf_device_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m         ):\n\u001b[0;32m-> 1044\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[0;31m# If it's a generation pipeline and the model can generate:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   4341\u001b[0m                     \u001b[0;34m\" `dtype` by passing the correct `dtype` argument.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4342\u001b[0m                 )\n\u001b[0;32m-> 4343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4345\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1369\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1371\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    955\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 957\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    958\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1355\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m                     )\n\u001b[0;32m-> 1357\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1358\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 3.38 MiB is free. Process 132218 has 22.15 GiB memory in use. Of the allocated memory 21.76 GiB is allocated by PyTorch, and 169.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stop Everything and Cleanup\n"
      ],
      "metadata": {
        "id": "t-pCMQfOOZGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this when you're done\n",
        "\n",
        "import os\n",
        "import signal\n",
        "\n",
        "# Kill streamlit\n",
        "!pkill -f streamlit\n",
        "\n",
        "# Kill ngrok tunnels\n",
        "ngrok.disconnect(public_url)\n",
        "\n",
        "print(\"‚úÖ Streamlit and ngrok stopped\")"
      ],
      "metadata": {
        "id": "nFJ-W97hBB4L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}