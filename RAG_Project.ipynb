{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "authorship_tag": "ABX9TyNWKG6BKbed56HPQ4zJ3gIC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ecc83c700b384be39dec743c206881ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b1d1b5dd2a784c4a85f3124cbc8419af",
              "IPY_MODEL_c8706eb35ba34fc3b6287c82be198366",
              "IPY_MODEL_881193dc3c704821a5b66b09d9cff314"
            ],
            "layout": "IPY_MODEL_976d5ae804794246823a41578abecc59"
          }
        },
        "b1d1b5dd2a784c4a85f3124cbc8419af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e30e36914e6e4ee08da51d19813b9c93",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_517fbe4605dd4dd99c08d2deaae6a8cf",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "c8706eb35ba34fc3b6287c82be198366": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be8682b8c5774349b467f1abdf18f92b",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fb3762ab7a88409a81174de860da0105",
            "value": 3
          }
        },
        "881193dc3c704821a5b66b09d9cff314": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_256a773de106413caa74151cec8f7200",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_3ba845c8387145088c3a825e7f606a6c",
            "value": "‚Äá3/3‚Äá[00:03&lt;00:00,‚Äá‚Äá1.00it/s]"
          }
        },
        "976d5ae804794246823a41578abecc59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e30e36914e6e4ee08da51d19813b9c93": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "517fbe4605dd4dd99c08d2deaae6a8cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "be8682b8c5774349b467f1abdf18f92b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb3762ab7a88409a81174de860da0105": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "256a773de106413caa74151cec8f7200": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ba845c8387145088c3a825e7f606a6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Spykabore15/students_RAG_project/blob/main/RAG_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mount Drive and Install Dependencies"
      ],
      "metadata": {
        "id": "UFSS07eYLleY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbWzdgjzAesk",
        "outputId": "fe1f7fc1-4159-473e-b261-9dc0f90a1a41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "‚úÖ All dependencies installed!\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install -q \\\n",
        "  langchain \\\n",
        "  langchain-community==0.2.* \\\n",
        "  faiss-cpu \\\n",
        "  sentence-transformers \\\n",
        "  transformers \\\n",
        "  accelerate \\\n",
        "  pypdf==4.* \\\n",
        "  streamlit \\\n",
        "  pyngrok\n",
        "\n",
        "print(\"‚úÖ All dependencies installed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set up ngrok"
      ],
      "metadata": {
        "id": "4oqa7JPlLhf9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up ngrok\n",
        "from pyngrok import ngrok\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Get ngrok token from secrets\n",
        "ngrok_token = userdata.get('NGROK_AUTH_KEY')\n",
        "ngrok.set_auth_token(ngrok_token)\n"
      ],
      "metadata": {
        "id": "I8Ea-O3sApkw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Streamit app configuration"
      ],
      "metadata": {
        "id": "ij6yy_CHLqlD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/app.py\n",
        "import streamlit as st\n",
        "from pathlib import Path\n",
        "import json\n",
        "from typing import List, Tuple\n",
        "import torch\n",
        "\n",
        "# Langchain imports\n",
        "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain import HuggingFacePipeline\n",
        "from langchain.prompts import PromptTemplate\n",
        "from transformers import pipeline\n",
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "# Page config\n",
        "st.set_page_config(\n",
        "    page_title=\"Enhanced Student RAG Chatbot\",\n",
        "    page_icon=\"üéì\",\n",
        "    layout=\"wide\"\n",
        ")\n",
        "\n",
        "# Constants\n",
        "DATA_PATH = \"/content/drive/MyDrive/TRAINING DATA\"\n",
        "\n",
        "# ==================== QUERY DECOMPOSITION ====================\n",
        "\n",
        "DECOMPOSITION_PROMPT = \"\"\"You are a query analyzer. Analyze if this question needs to be broken down into sub-questions.\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Rules:\n",
        "1. If the question is simple and focused, respond with: SIMPLE\n",
        "2. If the question has multiple parts or requires information from different sources, decompose it into 2-4 sub-questions.\n",
        "\n",
        "Response format for complex questions:\n",
        "SUB-QUERY 1: [first sub-question]\n",
        "SUB-QUERY 2: [second sub-question]\n",
        "SUB-QUERY 3: [third sub-question] (if needed)\n",
        "\n",
        "Examples:\n",
        "\n",
        "Question: \"Who is Dr. Sarah Chen?\"\n",
        "Response: SIMPLE\n",
        "\n",
        "Question: \"Which students worked with Dr. Sarah Chen on AI healthcare projects and what were their evaluation scores?\"\n",
        "Response:\n",
        "SUB-QUERY 1: Which students worked with Dr. Sarah Chen?\n",
        "SUB-QUERY 2: What AI healthcare projects exist in the database?\n",
        "SUB-QUERY 3: What were the evaluation scores for these students?\n",
        "\n",
        "Question: \"Compare robotics projects with environmental science projects\"\n",
        "Response:\n",
        "SUB-QUERY 1: What robotics projects are in the database?\n",
        "SUB-QUERY 2: What environmental science projects are in the database?\n",
        "\n",
        "Now analyze this question:\n",
        "Question: {question}\n",
        "Response:\"\"\"\n",
        "\n",
        "def decompose_query(llm, question: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Use LLM to decide if decomposition is needed\n",
        "    Returns: list of queries (original or decomposed)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        prompt = DECOMPOSITION_PROMPT.format(question=question)\n",
        "        response = llm.predict(prompt)\n",
        "\n",
        "        # Parse response\n",
        "        if \"SIMPLE\" in response:\n",
        "            return [question]\n",
        "\n",
        "        # Extract sub-queries\n",
        "        sub_queries = []\n",
        "        for line in response.split('\\n'):\n",
        "            if line.strip().startswith('SUB-QUERY'):\n",
        "                # Extract query after the colon\n",
        "                query = line.split(':', 1)[1].strip()\n",
        "                if query:\n",
        "                    sub_queries.append(query)\n",
        "\n",
        "        # Fallback: if parsing failed, use original\n",
        "        return sub_queries if sub_queries else [question]\n",
        "    except Exception as e:\n",
        "        st.warning(f\"Decomposition failed: {e}. Using original query.\")\n",
        "        return [question]\n",
        "\n",
        "# ==================== RERANKING ====================\n",
        "\n",
        "class CrossEncoderReranker:\n",
        "    \"\"\"Wrapper for cross-encoder reranking\"\"\"\n",
        "\n",
        "    def __init__(self, model_name=\"cross-encoder/ms-marco-MiniLM-L-6-v2\", device=\"cuda\"):\n",
        "        self.device = device\n",
        "        self.model = CrossEncoder(model_name, device=device)\n",
        "\n",
        "    def rerank(self, query: str, documents: List[Document], top_k: int = 4) -> List[Tuple[Document, float]]:\n",
        "        \"\"\"\n",
        "        Rerank documents based on relevance to query\n",
        "\n",
        "        Args:\n",
        "            query: User question\n",
        "            documents: List of Document objects\n",
        "            top_k: Number of top documents to return\n",
        "\n",
        "        Returns:\n",
        "            List of (document, score) tuples, sorted by score\n",
        "        \"\"\"\n",
        "        if not documents:\n",
        "            return []\n",
        "\n",
        "        # Prepare pairs for scoring\n",
        "        pairs = [[query, doc.page_content] for doc in documents]\n",
        "\n",
        "        # Get relevance scores\n",
        "        scores = self.model.predict(pairs)\n",
        "\n",
        "        # Combine documents with scores\n",
        "        doc_scores = list(zip(documents, scores))\n",
        "\n",
        "        # Sort by score (descending)\n",
        "        doc_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        return doc_scores[:top_k]\n",
        "\n",
        "# ==================== ENHANCED RAG PIPELINE ====================\n",
        "\n",
        "class EnhancedRAGPipeline:\n",
        "    \"\"\"Complete RAG pipeline with decomposition and reranking\"\"\"\n",
        "\n",
        "    def __init__(self, vectorstore, llm, reranker, retrieval_k=20, final_k=4):\n",
        "        self.vectorstore = vectorstore\n",
        "        self.llm = llm\n",
        "        self.reranker = reranker\n",
        "        self.retrieval_k = retrieval_k\n",
        "        self.final_k = final_k\n",
        "\n",
        "        # Retriever with higher k for reranking\n",
        "        self.retriever = vectorstore.as_retriever(\n",
        "            search_kwargs={\"k\": retrieval_k}\n",
        "        )\n",
        "\n",
        "    def retrieve_documents(self, sub_queries: List[str]) -> List[Document]:\n",
        "        \"\"\"\n",
        "        Retrieve documents for all sub-queries and deduplicate\n",
        "        \"\"\"\n",
        "        all_docs = []\n",
        "\n",
        "        # Retrieve for each sub-query\n",
        "        for sub_q in sub_queries:\n",
        "            docs = self.retriever.get_relevant_documents(sub_q)\n",
        "            all_docs.extend(docs)\n",
        "\n",
        "        # Deduplicate by content hash\n",
        "        unique_docs = []\n",
        "        seen_content = set()\n",
        "        for doc in all_docs:\n",
        "            content_hash = hash(doc.page_content)\n",
        "            if content_hash not in seen_content:\n",
        "                unique_docs.append(doc)\n",
        "                seen_content.add(content_hash)\n",
        "\n",
        "        return unique_docs\n",
        "\n",
        "    def generate_answer(self, question: str, context_docs: List[Tuple[Document, float]]) -> str:\n",
        "        \"\"\"\n",
        "        Generate final answer using LLM\n",
        "        \"\"\"\n",
        "        # Format context with sources\n",
        "        context = \"\\n\\n\".join([\n",
        "            f\"[Source {i+1} - Relevance: {score:.2f}]\\n{doc.page_content}\"\n",
        "            for i, (doc, score) in enumerate(context_docs)\n",
        "        ])\n",
        "\n",
        "        prompt = f\"\"\"You are a helpful assistant that answers using only the provided context.\n",
        "If the answer is not contained in the context, say \"I do not know\".\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer (be concise, structured, and cite sources when relevant):\"\"\"\n",
        "\n",
        "        answer = self.llm.predict(prompt)\n",
        "        return answer\n",
        "\n",
        "    def query(self, question: str, enable_decomposition: bool = True, enable_reranking: bool = True):\n",
        "        \"\"\"\n",
        "        Full RAG pipeline with all enhancements\n",
        "\n",
        "        Returns:\n",
        "            dict with answer, sub_queries, sources, and metadata\n",
        "        \"\"\"\n",
        "        # Step 1: Query Decomposition\n",
        "        if enable_decomposition:\n",
        "            sub_queries = decompose_query(self.llm, question)\n",
        "        else:\n",
        "            sub_queries = [question]\n",
        "\n",
        "        # Step 2: Retrieve documents\n",
        "        retrieved_docs = self.retrieve_documents(sub_queries)\n",
        "\n",
        "        # Step 3: Rerank documents\n",
        "        if enable_reranking and retrieved_docs:\n",
        "            # Use original question for reranking (not sub-queries)\n",
        "            reranked_docs = self.reranker.rerank(\n",
        "                query=question,\n",
        "                documents=retrieved_docs,\n",
        "                top_k=self.final_k\n",
        "            )\n",
        "        else:\n",
        "            # No reranking, just take top k\n",
        "            reranked_docs = [(doc, 1.0) for doc in retrieved_docs[:self.final_k]]\n",
        "\n",
        "        # Step 4: Generate answer\n",
        "        answer = self.generate_answer(question, reranked_docs)\n",
        "\n",
        "        return {\n",
        "            \"answer\": answer,\n",
        "            \"sub_queries\": sub_queries,\n",
        "            \"sources\": reranked_docs,\n",
        "            \"num_retrieved\": len(retrieved_docs),\n",
        "            \"num_sources\": len(reranked_docs)\n",
        "        }\n",
        "\n",
        "# ==================== DOCUMENT LOADING (CACHED) ====================\n",
        "\n",
        "@st.cache_resource\n",
        "def load_core_pdfs(data_path: Path) -> List[Document]:\n",
        "    \"\"\"Load core PDF documents with caching\"\"\"\n",
        "    core_pdfs = [\n",
        "        data_path / \"Projects.pdf\",\n",
        "        data_path / \"criteria.pdf\",\n",
        "        data_path / \"Mentors.pdf\"\n",
        "    ]\n",
        "\n",
        "    docs = []\n",
        "    for pdf in core_pdfs:\n",
        "        if pdf.exists():\n",
        "            pages = PyPDFLoader(str(pdf)).load()\n",
        "            for p in pages:\n",
        "                p.metadata.update({\"source\": pdf.name, \"category\": \"core\"})\n",
        "            docs.extend(pages)\n",
        "    return docs\n",
        "\n",
        "@st.cache_resource\n",
        "def load_student_dirs(data_path: Path) -> List[Document]:\n",
        "    \"\"\"Load student directories with caching\"\"\"\n",
        "    docs = []\n",
        "    students_dir = data_path / \"students\"\n",
        "\n",
        "    if not students_dir.exists():\n",
        "        return docs\n",
        "\n",
        "    for student_dir in students_dir.iterdir():\n",
        "        if not student_dir.is_dir():\n",
        "            continue\n",
        "\n",
        "        # Load student report\n",
        "        report_pdf = student_dir / \"report.pdf\"\n",
        "        if report_pdf.exists():\n",
        "            pages = PyPDFLoader(str(report_pdf)).load()\n",
        "            for p in pages:\n",
        "                p.metadata.update({\n",
        "                    \"source\": f\"{student_dir.name}/report.pdf\",\n",
        "                    \"student\": student_dir.name,\n",
        "                    \"category\": \"student_report\"\n",
        "                })\n",
        "            docs.extend(pages)\n",
        "\n",
        "        # Load student summary\n",
        "        summary_txt = student_dir / \"summary.txt\"\n",
        "        if summary_txt.exists():\n",
        "            tdocs = TextLoader(str(summary_txt), encoding=\"utf-8\").load()\n",
        "            for d in tdocs:\n",
        "                d.metadata.update({\n",
        "                    \"source\": f\"{student_dir.name}/summary.txt\",\n",
        "                    \"student\": student_dir.name,\n",
        "                    \"category\": \"student_summary\"\n",
        "                })\n",
        "            docs.extend(tdocs)\n",
        "\n",
        "        # Load student metadata\n",
        "        meta_json = student_dir / \"metadata.json\"\n",
        "        if meta_json.exists():\n",
        "            try:\n",
        "                meta = json.loads(meta_json.read_text(encoding=\"utf-8\"))\n",
        "                meta_doc = Document(\n",
        "                    page_content=json.dumps(meta, ensure_ascii=False, indent=2),\n",
        "                    metadata={\n",
        "                        \"source\": f\"{student_dir.name}/metadata.json\",\n",
        "                        \"student\": student_dir.name,\n",
        "                        \"category\": \"student_metadata\"\n",
        "                    }\n",
        "                )\n",
        "                docs.append(meta_doc)\n",
        "            except Exception as e:\n",
        "                st.warning(f\"Couldn't parse {meta_json}: {e}\")\n",
        "\n",
        "    return docs\n",
        "\n",
        "# ==================== MAIN INITIALIZATION (CACHED) ====================\n",
        "\n",
        "@st.cache_resource\n",
        "def initialize_enhanced_rag():\n",
        "    \"\"\"Initialize the complete enhanced RAG system with caching\"\"\"\n",
        "\n",
        "    # Check GPU availability\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    st.info(f\"üñ•Ô∏è Using device: **{device}**\")\n",
        "\n",
        "    if device == \"cpu\":\n",
        "        st.warning(\"‚ö†Ô∏è GPU not detected! This will be slower. Enable GPU in Colab Runtime settings.\")\n",
        "\n",
        "    with st.spinner(\"üìö Loading documents...\"):\n",
        "        data_path = Path(DATA_PATH)\n",
        "        raw_docs = load_core_pdfs(data_path) + load_student_dirs(data_path)\n",
        "        st.success(f\"‚úÖ Loaded **{len(raw_docs)}** documents\")\n",
        "\n",
        "    with st.spinner(\" Splitting documents into chunks...\"):\n",
        "        splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=1000,\n",
        "            chunk_overlap=150,\n",
        "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        "        )\n",
        "        chunks = splitter.split_documents(raw_docs)\n",
        "        st.success(f\"‚úÖ Created **{len(chunks)}** chunks\")\n",
        "\n",
        "    with st.spinner(\"Loading embedding model...\"):\n",
        "        embeddings = HuggingFaceEmbeddings(\n",
        "            model_name=\"all-MiniLM-L6-v2\",\n",
        "            model_kwargs={'device': device}\n",
        "        )\n",
        "        st.success(\"‚úÖ Embeddings model loaded\")\n",
        "\n",
        "    with st.spinner(\" Building vector store...\"):\n",
        "        vectorstore = FAISS.from_documents(\n",
        "            documents=chunks,\n",
        "            embedding=embeddings\n",
        "        )\n",
        "        st.success(\"‚úÖ Vector store created\")\n",
        "\n",
        "    with st.spinner(\"ü§ñ Loading language model (Mistral-7B)...\"):\n",
        "        pipe = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "            device=0 if device == \"cuda\" else -1,\n",
        "            torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        "            max_new_tokens=300,\n",
        "            temperature=0.1,\n",
        "            return_full_text=False\n",
        "        )\n",
        "        llm = HuggingFacePipeline(pipeline=pipe)\n",
        "        st.success(f\"‚úÖ Language model loaded on **{device}**\")\n",
        "\n",
        "    with st.spinner(\"üéØ Loading reranker (Cross-Encoder)...\"):\n",
        "        reranker = CrossEncoderReranker(device=device)\n",
        "        st.success(\"‚úÖ Reranker loaded\")\n",
        "\n",
        "    # Create enhanced RAG pipeline\n",
        "    rag = EnhancedRAGPipeline(\n",
        "        vectorstore=vectorstore,\n",
        "        llm=llm,\n",
        "        reranker=reranker,\n",
        "        retrieval_k=20,\n",
        "        final_k=4\n",
        "    )\n",
        "\n",
        "    st.success(\" **Enhanced RAG system initialized!**\")\n",
        "\n",
        "    return rag\n",
        "\n",
        "# ==================== STREAMLIT UI ====================\n",
        "\n",
        "def main():\n",
        "    st.title(\"üéì Enhanced Student Project RAG Chatbot\")\n",
        "    st.markdown(\"\"\"\n",
        "    Ask questions about student projects, mentors, and evaluation criteria.\n",
        "    **New Features:** Query decomposition + Advanced reranking\n",
        "    \"\"\")\n",
        "\n",
        "    # Sidebar configuration\n",
        "    with st.sidebar:\n",
        "        st.header(\"‚öôÔ∏è Configuration\")\n",
        "\n",
        "        enable_decomposition = st.checkbox(\n",
        "            \"Enable Query Decomposition\",\n",
        "            value=True,\n",
        "            help=\"Break complex questions into simpler sub-queries\"\n",
        "        )\n",
        "\n",
        "        enable_reranking = st.checkbox(\n",
        "            \"Enable Reranking\",\n",
        "            value=True,\n",
        "            help=\"Reorder retrieved documents by relevance\"\n",
        "        )\n",
        "\n",
        "        show_details = st.checkbox(\n",
        "            \"Show Processing Details\",\n",
        "            value=True,\n",
        "            help=\"Display sub-queries and relevance scores\"\n",
        "        )\n",
        "\n",
        "        st.markdown(\"---\")\n",
        "        st.markdown(\"### üí° Example Questions\")\n",
        "        st.markdown(\"\"\"\n",
        "        **Simple:**\n",
        "        - Who are the mentors for robotics projects?\n",
        "        - What is Dr. Sarah Chen's expertise?\n",
        "\n",
        "        **Complex (will decompose):**\n",
        "        - Which students worked with Dr. Chen on AI healthcare projects and what were their scores?\n",
        "        - Compare robotics projects with environmental science projects\n",
        "        - List students who can collaborate on Robot and Air Quality projects\n",
        "        \"\"\")\n",
        "\n",
        "        st.markdown(\"---\")\n",
        "        st.markdown(\"### System Info\")\n",
        "        if torch.cuda.is_available():\n",
        "            st.success(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "        else:\n",
        "            st.error(\"‚ùå No GPU detected\")\n",
        "\n",
        "    # Initialize RAG system\n",
        "    try:\n",
        "        rag = initialize_enhanced_rag()\n",
        "    except Exception as e:\n",
        "        st.error(f\"‚ùå Error initializing RAG system: {e}\")\n",
        "        st.stop()\n",
        "\n",
        "    # Chat interface\n",
        "    if \"messages\" not in st.session_state:\n",
        "        st.session_state.messages = []\n",
        "\n",
        "    # Display chat history\n",
        "    for message in st.session_state.messages:\n",
        "        with st.chat_message(message[\"role\"]):\n",
        "            st.markdown(message[\"content\"])\n",
        "\n",
        "            # Show details if available\n",
        "            if \"details\" in message and show_details:\n",
        "                with st.expander(\"üîç Processing Details\"):\n",
        "                    st.markdown(message[\"details\"])\n",
        "\n",
        "            # Show sources if available\n",
        "            if \"sources\" in message:\n",
        "                with st.expander(\"üìö Source Documents\"):\n",
        "                    st.markdown(message[\"sources\"])\n",
        "\n",
        "    # Chat input\n",
        "    if question := st.chat_input(\"Ask a question about student projects...\"):\n",
        "        # Add user message\n",
        "        st.session_state.messages.append({\"role\": \"user\", \"content\": question})\n",
        "        with st.chat_message(\"user\"):\n",
        "            st.markdown(question)\n",
        "\n",
        "        # Get response\n",
        "        with st.chat_message(\"assistant\"):\n",
        "            with st.spinner(\"ü§î Processing your question...\"):\n",
        "                try:\n",
        "                    # Query the RAG system\n",
        "                    result = rag.query(\n",
        "                        question=question,\n",
        "                        enable_decomposition=enable_decomposition,\n",
        "                        enable_reranking=enable_reranking\n",
        "                    )\n",
        "\n",
        "                    # Display answer\n",
        "                    st.markdown(result[\"answer\"])\n",
        "\n",
        "                    # Format processing details\n",
        "                    details_text = \"\"\n",
        "                    if show_details:\n",
        "                        if len(result[\"sub_queries\"]) > 1:\n",
        "                            details_text += \"**üîç Query Decomposition:**\\n\"\n",
        "                            for i, sq in enumerate(result[\"sub_queries\"], 1):\n",
        "                                details_text += f\"{i}. {sq}\\n\"\n",
        "                            details_text += \"\\n\"\n",
        "\n",
        "                        details_text += f\"**Retrieval Stats:**\\n\"\n",
        "                        details_text += f\"- Retrieved: {result['num_retrieved']} documents\\n\"\n",
        "                        details_text += f\"- After reranking: {result['num_sources']} documents\\n\"\n",
        "\n",
        "                        with st.expander(\"üîç Processing Details\"):\n",
        "                            st.markdown(details_text)\n",
        "\n",
        "                    # Format sources\n",
        "                    sources_text = \"\"\n",
        "                    for i, (doc, score) in enumerate(result[\"sources\"], 1):\n",
        "                        source = doc.metadata.get('source', 'Unknown')\n",
        "                        category = doc.metadata.get('category', 'N/A')\n",
        "                        student = doc.metadata.get('student', 'N/A')\n",
        "\n",
        "                        sources_text += f\"**{i}. {source}** (Score: {score:.3f})\\n\"\n",
        "                        sources_text += f\"   - Category: {category}\\n\"\n",
        "                        if student != 'N/A':\n",
        "                            sources_text += f\"   - Student: {student}\\n\"\n",
        "                        sources_text += f\"   - Content preview: {doc.page_content[:200]}...\\n\\n\"\n",
        "\n",
        "                    with st.expander(\"üìö Source Documents\"):\n",
        "                        st.markdown(sources_text)\n",
        "\n",
        "                    # Add to chat history\n",
        "                    st.session_state.messages.append({\n",
        "                        \"role\": \"assistant\",\n",
        "                        \"content\": result[\"answer\"],\n",
        "                        \"details\": details_text if show_details else None,\n",
        "                        \"sources\": sources_text\n",
        "                    })\n",
        "\n",
        "                except Exception as e:\n",
        "                    error_msg = f\"‚ùå Error generating response: {str(e)}\"\n",
        "                    st.error(error_msg)\n",
        "                    st.session_state.messages.append({\n",
        "                        \"role\": \"assistant\",\n",
        "                        \"content\": error_msg\n",
        "                    })\n",
        "\n",
        "    # Action buttons in sidebar\n",
        "    with st.sidebar:\n",
        "        st.markdown(\"---\")\n",
        "        col1, col2 = st.columns(2)\n",
        "\n",
        "        with col1:\n",
        "            if st.button(\"üóëÔ∏è Clear Chat\", use_container_width=True):\n",
        "                st.session_state.messages = []\n",
        "                st.rerun()\n",
        "\n",
        "        with col2:\n",
        "            if st.button(\"üîÑ Reload System\", use_container_width=True):\n",
        "                st.cache_resource.clear()\n",
        "                st.rerun()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "5_AWoF4lAtUI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "726947eb-ac52-40de-aa78-cb8e9fd3df1c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optional - Pre-build and Save Vector Store\n",
        "\n",
        "Run this once to save time"
      ],
      "metadata": {
        "id": "rhPcUHToJDfR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import torch\n",
        "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "import json\n",
        "\n",
        "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
        "\n",
        "DATA_PATH = \"/content/drive/MyDrive/TRAINING DATA\"\n",
        "VECTORSTORE_PATH = \"/content/drive/MyDrive/vectorstore_enhanced\"\n",
        "\n",
        "def load_all_documents():\n",
        "    \"\"\"Load all documents from the data directory\"\"\"\n",
        "    data_path = Path(DATA_PATH)\n",
        "    docs = []\n",
        "\n",
        "    # Load core PDFs\n",
        "    core_pdfs = [\n",
        "        data_path / \"Projects.pdf\",\n",
        "        data_path / \"criteria.pdf\",\n",
        "        data_path / \"Mentors.pdf\"\n",
        "    ]\n",
        "\n",
        "    for pdf in core_pdfs:\n",
        "        if pdf.exists():\n",
        "            pages = PyPDFLoader(str(pdf)).load()\n",
        "            for p in pages:\n",
        "                p.metadata.update({\"source\": pdf.name, \"category\": \"core\"})\n",
        "            docs.extend(pages)\n",
        "\n",
        "    # Load student directories\n",
        "    students_dir = data_path / \"students\"\n",
        "    if students_dir.exists():\n",
        "        for student_dir in students_dir.iterdir():\n",
        "            if not student_dir.is_dir():\n",
        "                continue\n",
        "\n",
        "            # Report PDF\n",
        "            report_pdf = student_dir / \"report.pdf\"\n",
        "            if report_pdf.exists():\n",
        "                pages = PyPDFLoader(str(report_pdf)).load()\n",
        "                for p in pages:\n",
        "                    p.metadata.update({\n",
        "                        \"source\": f\"{student_dir.name}/report.pdf\",\n",
        "                        \"student\": student_dir.name,\n",
        "                        \"category\": \"student_report\"\n",
        "                    })\n",
        "                docs.extend(pages)\n",
        "\n",
        "            # Summary TXT\n",
        "            summary_txt = student_dir / \"summary.txt\"\n",
        "            if summary_txt.exists():\n",
        "                tdocs = TextLoader(str(summary_txt), encoding=\"utf-8\").load()\n",
        "                for d in tdocs:\n",
        "                    d.metadata.update({\n",
        "                        \"source\": f\"{student_dir.name}/summary.txt\",\n",
        "                        \"student\": student_dir.name,\n",
        "                        \"category\": \"student_summary\"\n",
        "                    })\n",
        "                docs.extend(tdocs)\n",
        "\n",
        "            # Metadata JSON\n",
        "            meta_json = student_dir / \"metadata.json\"\n",
        "            if meta_json.exists():\n",
        "                try:\n",
        "                    meta = json.loads(meta_json.read_text(encoding=\"utf-8\"))\n",
        "                    meta_doc = Document(\n",
        "                        page_content=json.dumps(meta, ensure_ascii=False, indent=2),\n",
        "                        metadata={\n",
        "                            \"source\": f\"{student_dir.name}/metadata.json\",\n",
        "                            \"student\": student_dir.name,\n",
        "                            \"category\": \"student_metadata\"\n",
        "                        }\n",
        "                    )\n",
        "                    docs.append(meta_doc)\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ö†Ô∏è Couldn't parse {meta_json}: {e}\")\n",
        "\n",
        "    return docs\n",
        "\n",
        "# Load documents\n",
        "print(\"üìö Loading documents...\")\n",
        "raw_docs = load_all_documents()\n",
        "print(f\"‚úÖ Loaded {len(raw_docs)} documents\")\n",
        "\n",
        "# Split into chunks\n",
        "print(\"Splitting documents...\")\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=150,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        ")\n",
        "chunks = splitter.split_documents(raw_docs)\n",
        "print(f\"‚úÖ Created {len(chunks)} chunks\")\n",
        "\n",
        "# Create embeddings\n",
        "print(\" Creating embeddings...\")\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"all-MiniLM-L6-v2\",\n",
        "    model_kwargs={'device': device}\n",
        ")\n",
        "print(f\"‚úÖ Embeddings model loaded on {device}\")\n",
        "\n",
        "# Build vector store\n",
        "print(\"Building vector store (this may take 2-3 minutes)...\")\n",
        "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
        "print(\"‚úÖ Vector store created!\")\n",
        "\n",
        "# Save to disk\n",
        "print(f\" Saving vector store to {VECTORSTORE_PATH}...\")\n",
        "vectorstore.save_local(VECTORSTORE_PATH)\n",
        "print(\"‚úÖ Vector store saved! You can now skip this cell in future runs.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwjkCIutHWW1",
        "outputId": "03c978f4-c81b-41eb-bbda-e05584b235c2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Available: True\n",
            "üìö Loading documents...\n",
            "‚úÖ Loaded 134 documents\n",
            "Splitting documents...\n",
            "‚úÖ Created 287 chunks\n",
            " Creating embeddings...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-389712022.py:102: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
            "  embeddings = HuggingFaceEmbeddings(\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Embeddings model loaded on cuda\n",
            "Building vector store (this may take 2-3 minutes)...\n",
            "‚úÖ Vector store created!\n",
            " Saving vector store to /content/drive/MyDrive/vectorstore_enhanced...\n",
            "‚úÖ Vector store saved! You can now skip this cell in future runs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run the app with ngrok"
      ],
      "metadata": {
        "id": "UI7UkiWuL0SO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import time\n",
        "import requests\n",
        "\n",
        "# Kill any existing streamlit processes\n",
        "!pkill -f streamlit\n",
        "\n",
        "# Wait a moment for cleanup\n",
        "time.sleep(2)\n",
        "\n",
        "# Start Streamlit in background\n",
        "print(\"Starting Streamlit...\")\n",
        "streamlit_process = subprocess.Popen(\n",
        "    [\"streamlit\", \"run\", \"/content/app.py\",\n",
        "     \"--server.port=8501\",\n",
        "     \"--server.address=localhost\",  # Changed from 0.0.0.0\n",
        "     \"--server.headless=true\"],\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.PIPE\n",
        ")\n",
        "\n",
        "# Wait for Streamlit to actually start (checking if it's ready)\n",
        "print(\"‚è≥ Waiting for Streamlit to initialize...\")\n",
        "max_retries = 5\n",
        "for i in range(max_retries):\n",
        "    try:\n",
        "        response = requests.get(\"http://localhost:8501\")\n",
        "        if response.status_code == 200:\n",
        "            print(\"‚úÖ Streamlit is ready!\")\n",
        "            break\n",
        "    except:\n",
        "        pass\n",
        "    time.sleep(2)\n",
        "    print(f\"   Attempt {i+1}/{max_retries}...\")\n",
        "else:\n",
        "    print(\"‚ùå Streamlit failed to start. Check logs below:\")\n",
        "    print(streamlit_process.stderr.read().decode())\n",
        "    raise Exception(\"Streamlit didn't start properly\")\n",
        "\n",
        "# Now create ngrok tunnel\n",
        "print(\" Creating ngrok tunnel...\")\n",
        "public_url = ngrok.connect(8501)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\" Enhanced RAG Chatbot is LIVE!\")\n",
        "print(\"=\" * 60)\n",
        "print(f\" Public URL: {public_url}\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\n‚úÖ Features Enabled:\")\n",
        "print(\"  - Query Decomposition (LLM-based)\")\n",
        "print(\"  - Cross-Encoder Reranking\")\n",
        "print(\"  - Enhanced Retrieval (k=20 ‚Üí top 4)\")\n",
        "print(\"  - Source Attribution with Scores\")\n",
        "print(\"\\n‚ö†Ô∏è Keep this cell running to maintain the connection\")\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "id": "fX1PYcWIA5w5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97b09747-25e1-47af-8158-585fe76d85fd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Streamlit...\n",
            "‚è≥ Waiting for Streamlit to initialize...\n",
            "   Attempt 1/5...\n",
            "‚úÖ Streamlit is ready!\n",
            " Creating ngrok tunnel...\n",
            "============================================================\n",
            " Enhanced RAG Chatbot is LIVE!\n",
            "============================================================\n",
            " Public URL: NgrokTunnel: \"https://unmustered-lacresha-tetratomic.ngrok-free.dev\" -> \"http://localhost:8501\"\n",
            "============================================================\n",
            "\n",
            "‚úÖ Features Enabled:\n",
            "  - Query Decomposition (LLM-based)\n",
            "  - Cross-Encoder Reranking\n",
            "  - Enhanced Retrieval (k=20 ‚Üí top 4)\n",
            "  - Source Attribution with Scores\n",
            "\n",
            "‚ö†Ô∏è Keep this cell running to maintain the connection\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optional - Test the System Directly in Notebook"
      ],
      "metadata": {
        "id": "Ik8XVK8iOjRC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test individual components before running Streamlit\n",
        "\n",
        "from transformers import pipeline\n",
        "from langchain import HuggingFacePipeline\n",
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "# Load models\n",
        "print(\"Loading models for testing...\")\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "    device=device,\n",
        "    torch_dtype=torch.float16 if device >= 0 else torch.float32,\n",
        "    max_new_tokens=100,\n",
        "    return_full_text=False\n",
        ")\n",
        "llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "# Test decomposition\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Testing Query Decomposition\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "test_questions = [\n",
        "    \"Who is Dr. Sarah Chen?\",\n",
        "    \"Which students worked with Dr. Chen and what were their scores?\",\n",
        "    \"Compare robotics and environmental projects\"\n",
        "]\n",
        "\n",
        "DECOMP_PROMPT = \"\"\"You are a query analyzer. Analyze if this question needs to be broken down.\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Rules:\n",
        "1. If simple, respond with: SIMPLE\n",
        "2. If complex, decompose into sub-questions.\n",
        "\n",
        "Response format:\n",
        "SUB-QUERY 1: [question]\n",
        "SUB-QUERY 2: [question]\n",
        "\n",
        "Now analyze:\n",
        "Question: {question}\n",
        "Response:\"\"\"\n",
        "\n",
        "for q in test_questions:\n",
        "    prompt = DECOMP_PROMPT.format(question=q)\n",
        "    response = llm.predict(prompt)\n",
        "    print(f\"\\nQ: {q}\")\n",
        "    print(f\"Response: {response[:200]}\")\n",
        "\n",
        "# Test reranker\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Testing Cross-Encoder Reranker\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
        "\n",
        "query = \"AI healthcare projects\"\n",
        "test_docs = [\n",
        "    \"This project focuses on AI-powered medical diagnosis using deep learning.\",\n",
        "    \"Students worked on environmental monitoring systems.\",\n",
        "    \"The healthcare AI system achieved 95% accuracy in disease prediction.\",\n",
        "    \"Robotics project for autonomous delivery in hospitals.\"\n",
        "]\n",
        "\n",
        "pairs = [[query, doc] for doc in test_docs]\n",
        "scores = reranker.predict(pairs)\n",
        "\n",
        "print(f\"Query: {query}\\n\")\n",
        "for doc, score in sorted(zip(test_docs, scores), key=lambda x: x[1], reverse=True):\n",
        "    print(f\"Score: {score:.3f} - {doc[:60]}...\")\n",
        "\n",
        "print(\"\\n‚úÖ All components working! Ready to use Streamlit app.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 854,
          "referenced_widgets": [
            "ecc83c700b384be39dec743c206881ff",
            "b1d1b5dd2a784c4a85f3124cbc8419af",
            "c8706eb35ba34fc3b6287c82be198366",
            "881193dc3c704821a5b66b09d9cff314",
            "976d5ae804794246823a41578abecc59",
            "e30e36914e6e4ee08da51d19813b9c93",
            "517fbe4605dd4dd99c08d2deaae6a8cf",
            "be8682b8c5774349b467f1abdf18f92b",
            "fb3762ab7a88409a81174de860da0105",
            "256a773de106413caa74151cec8f7200",
            "3ba845c8387145088c3a825e7f606a6c"
          ]
        },
        "id": "HSglcyJRH_Yp",
        "outputId": "6415721f-f695-43c8-b11b-ca25ca8fb67d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading models for testing...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ecc83c700b384be39dec743c206881ff"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "/tmp/ipython-input-1269056518.py:19: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFacePipeline`.\n",
            "  llm = HuggingFacePipeline(pipeline=pipe)\n",
            "/tmp/ipython-input-1269056518.py:50: LangChainDeprecationWarning: The method `BaseLLM.predict` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use invoke instead.\n",
            "  response = llm.predict(prompt)\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Testing Query Decomposition\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Q: Who is Dr. Sarah Chen?\n",
            "Response: \n",
            "This question is simple and does not require decomposition.\n",
            "\n",
            "SIMPLE.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Q: Which students worked with Dr. Chen and what were their scores?\n",
            "Response: \n",
            "SUB-QUERY 1: Identify students who worked with Dr. Chen.\n",
            "SUB-QUERY 2: Get scores for the identified students.\n",
            "\n",
            "Q: Compare robotics and environmental projects\n",
            "Response: \n",
            "This question is complex as it involves comparing two distinct areas: robotics and environmental projects.\n",
            "\n",
            "To effectively compare these two areas, we need to break down the question into sub-queries\n",
            "\n",
            "============================================================\n",
            "Testing Cross-Encoder Reranker\n",
            "============================================================\n",
            "Query: AI healthcare projects\n",
            "\n",
            "Score: 5.700 - This project focuses on AI-powered medical diagnosis using d...\n",
            "Score: 2.920 - The healthcare AI system achieved 95% accuracy in disease pr...\n",
            "Score: -2.748 - Robotics project for autonomous delivery in hospitals....\n",
            "Score: -11.337 - Students worked on environmental monitoring systems....\n",
            "\n",
            "‚úÖ All components working! Ready to use Streamlit app.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stop Everything and Cleanup\n"
      ],
      "metadata": {
        "id": "t-pCMQfOOZGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this when you're done\n",
        "\n",
        "import os\n",
        "import signal\n",
        "\n",
        "# Kill streamlit\n",
        "!pkill -f streamlit\n",
        "\n",
        "# Kill ngrok tunnels\n",
        "ngrok.disconnect(public_url)\n",
        "\n",
        "print(\"‚úÖ Streamlit and ngrok stopped\")"
      ],
      "metadata": {
        "id": "nFJ-W97hBB4L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc0bc469-4a67-416e-aaf2-b10fa78fa341"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Streamlit and ngrok stopped\n"
          ]
        }
      ]
    }
  ]
}