{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "authorship_tag": "ABX9TyOqp88nY6u9GQwNlPEcmqHt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Spykabore15/students_RAG_project/blob/main/Simple_RAG_Students_database.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dependencies installation"
      ],
      "metadata": {
        "id": "8vigslkWaV7d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- langchain: Main LLM orchestration framework\n",
        "- langchain-community: community extentions and integrations\n",
        "- langchain-ebeddings: HaggingFace integration for ebeddings\n",
        "- langchain-faiss: vectorstore connector for FAISS\n",
        "- FAISS: vectorbase\n",
        "- pypdf: PDF parser to extract text from documents\n"
      ],
      "metadata": {
        "id": "OVchFa87aojz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "y7FzvZgnaGgy",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b79ed0c-9a67-419c-c598-b2072e98e169"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install -q \\\n",
        "  langchain \\\n",
        "  langchain-community==0.2.* \\\n",
        "  faiss-cpu \\\n",
        "  sentence-transformers \\\n",
        "  transformers \\\n",
        "  accelerate \\\n",
        "  pypdf==4.* \\\n",
        "  streamlit \\\n",
        "  pyngrok"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step2\n",
        "\n",
        "Import essentials libraries for RAG pipeline including documents ingestion, text splitting, embeddings generation and vector database management."
      ],
      "metadata": {
        "id": "QzK65xj1fPnQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Standard utilities\n",
        "from pathlib import Path\n",
        "import json\n",
        "from typing import List\n",
        "\n",
        "# Langchain components for HaggingFace integration\n",
        "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "# Langchain components for LLM orchestration\n",
        "from langchain import HuggingFacePipeline\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "79sGRPQhgwAx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3\n",
        "Load and Structure Core and Students Documents\n",
        "\n",
        "In this step, we define utility functions to load and structure all the documents required for the RAG pipeline. This includes:\n",
        "\n",
        "Loading core reference PDFs such as projects, criteria, and mentors.\n",
        "\n",
        "Iterating through student folders to load individual reports, summaries, and metadata.\n",
        "\n",
        "Tagging each document with relevant metadata (like source, student, and category) to support context-aware retrieval later."
      ],
      "metadata": {
        "id": "kysvChgWiQVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path =\"/content/drive/MyDrive/TRAINING DATA\"\n",
        "DATA = Path(path)"
      ],
      "metadata": {
        "id": "znuJZ1kHiqHF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Core documents that serves as global references material\n",
        "\n",
        "CORE_PDFS= [\n",
        "    DATA / \"Projects.pdf\",\n",
        "    DATA / \"criteria.pdf\",\n",
        "    DATA / \"Mentors.pdf\"\n",
        "]\n",
        "\n",
        "def load_core_pdfs() -> List[Document]:\n",
        "  \"\"\"\n",
        "    Loads all core PDF documents (projects, criteria, mentors).\n",
        "    Each page is converted into a LangChain Document with metadata attached.\n",
        "  \"\"\"\n",
        "\n",
        "  docs = []\n",
        "  for pdf in CORE_PDFS:\n",
        "    if pdf.exists():\n",
        "      # Load all pages from the PDF\n",
        "      pages= PyPDFLoader(str(pdf)).load()\n",
        "      # Add source metadata for traceability\n",
        "      for p in pages:\n",
        "        p.metadata.update({\"source\": pdf.name, \"category\": \"core\"})\n",
        "      docs.extend(pages)\n",
        "  return docs\n",
        "\n"
      ],
      "metadata": {
        "id": "95n2Va3YlVWD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_student_dirs() -> List[Document]:\n",
        "  \"\"\"\n",
        "    Iterates through each student directory and loads:\n",
        "    - report.pdf: main project report\n",
        "    - summary.txt: short text summary (if exists)\n",
        "    - metadata.json: student metadata (if exists)\n",
        "    Each file is wrapped in a Document object with descriptive metadata.\n",
        "    \"\"\"\n",
        "  docs = []\n",
        "  students_dir = DATA / \"students\"\n",
        "  if not students_dir.exists():\n",
        "    return docs\n",
        "  for student_dir in students_dir.iterdir():\n",
        "    if not student_dir.is_dir():\n",
        "      continue\n",
        "\n",
        "    # Load student report\n",
        "    report_pdf = student_dir / \"report.pdf\"\n",
        "    if report_pdf.exists():\n",
        "      pages = PyPDFLoader(str(report_pdf)).load()\n",
        "      for p in pages:\n",
        "        p.metadata.update({\n",
        "            \"source\": f\"{student_dir.name}/report.pdf\",\n",
        "            \"student\": student_dir.name,\n",
        "            \"category\": \"student_report\"\n",
        "        })\n",
        "      docs.extend(pages)\n",
        "\n",
        "    # Load Student summaru (TXT)\n",
        "    summary_txt = student_dir / \"summary.txt\"\n",
        "    if summary_txt.exists():\n",
        "      tdocs = TextLoader(str(summary_txt), encoding= \"utf-8\").load()\n",
        "      for d in tdocs:\n",
        "        d.metadata.update({\n",
        "          \"source\": f\"{student_dir.name}/summary.txt\",\n",
        "          \"student\": student_dir.name,\n",
        "          \"category\": \"student_summary\"\n",
        "        })\n",
        "      docs.extend(tdocs)\n",
        "\n",
        "    # Load Student meatadata(JSON)\n",
        "    meta_json = student_dir / \"metadata.json\"\n",
        "    if meta_json.exists():\n",
        "      try:\n",
        "        meta = json.loads(meta_json.read_text(encoding=\"utf-8\"))\n",
        "        meta_doc = Document(\n",
        "            page_content = json.dumps(meta, ensure_ascii=False, indent=2),\n",
        "            metadata={\n",
        "                \"source\": f\"{student_dir.name}/metadata.json\",\n",
        "                \"student\": student_dir.name,\n",
        "                \"category\": \"student_metadata\"\n",
        "            }\n",
        "        )\n",
        "        docs.append(meta_doc)\n",
        "      except Exception as e:\n",
        "        print(f\"Couln't parse {meta_json}: {e}\")\n",
        "\n",
        "  return docs"
      ],
      "metadata": {
        "id": "4mZpm5rQ7hLX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine all loaded documents from core and student sources\n",
        "raw_docs = load_core_pdfs() + load_student_dirs()\n",
        "\n",
        "# Display how many total document objects were loaded\n",
        "print(f\"Loaded {len(raw_docs)} raw documents (pages + text).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9mokZQq7jlK",
        "outputId": "d5cfdda7-1da9-43e9-e8ea-f4593a8e43e9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 134 raw documents (pages + text).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4\n",
        "Split Documents into Manageable Chunks\n",
        "\n",
        "In this step, we use a recursive character text splitter to break down long documents into smaller, overlapping chunks. This ensures that each chunk is within token limits and still preserves contextual continuity for embeddings and retrieval.\n",
        "\n",
        "chunk_size=1000: Maximum number of characters in a single chunk.\n",
        "\n",
        "chunk_overlap=150: Overlap between consecutive chunks to maintain context across boundaries.\n",
        "\n",
        "separators: Defines the preferred order of splitting (paragraphs → lines → words → characters).\n",
        "\n",
        "Finally, we print the number of generated chunks and preview one sample with its metadata for verification."
      ],
      "metadata": {
        "id": "9POIiB207tWb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the RecursiveCharacterTextSplitter with chunking parameters.\n",
        "# It prioritizes larger splits first (paragraphs, then sentences, etc.)\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 1000,\n",
        "    chunk_overlap = 150,\n",
        "    separators = [\"\\n\\n\", \"\\n\", \" \", \"\"], # Logical split hierarchy (paragraph > line > word > char)\n",
        ")\n",
        "\n",
        "# Apply the splitter to the combined raw documents\n",
        "chunks = splitter.split_documents(raw_docs)\n",
        "\n",
        "# Display how many text chunks were created\n",
        "print(f\"Created {len(chunks)} chunks.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VqNMfOvgBEAL",
        "outputId": "43cc980a-b3df-43ff-a23e-7e2c314f4701"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 287 chunks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preview the first chunk to verify splitting and metadata tagging\n",
        "print(chunks[0].page_content[:300], \"...\\n\", chunks[0].metadata)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5W2sCF2EpdM",
        "outputId": "b47f9b7b-0f5b-4d0b-80df-386af28433a3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "UNIVERSITY\n",
            " \n",
            "PROJECT\n",
            " \n",
            "GUIDELINES\n",
            " \n",
            "2024\n",
            " \n",
            " \n",
            "CAPSTONE\n",
            " \n",
            "PROJECT\n",
            " \n",
            "REQUIREMENTS\n",
            " \n",
            " \n",
            "1.\n",
            " \n",
            "PROJECT\n",
            " \n",
            "DURATION\n",
            " \n",
            "AND\n",
            " \n",
            "MILESTONES\n",
            " \n",
            "   \n",
            "-\n",
            " \n",
            "Project\n",
            " \n",
            "Duration:\n",
            " \n",
            "12\n",
            " \n",
            "weeks\n",
            " \n",
            "minimum\n",
            " \n",
            "   \n",
            "-\n",
            " \n",
            "Proposal\n",
            " \n",
            "Submission:\n",
            " \n",
            "Week\n",
            " \n",
            "1\n",
            " \n",
            "   \n",
            "-\n",
            " \n",
            "Mid-term\n",
            " \n",
            "Review:\n",
            " \n",
            "Week\n",
            " \n",
            "6\n",
            " \n",
            "   \n",
            "-\n",
            " \n",
            "Final\n",
            " \n",
            "Submission:\n",
            " \n",
            "Week\n",
            " ...\n",
            " {'source': 'Projects.pdf', 'page': 0, 'category': 'core'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 6\n",
        "Generate embeddings and store them in  FAISS\n",
        "\n",
        "In this step, we initialize the embedding model and FAISS vector database, then index all text chunks for retrieval. This forms the foundation of the RAG system — enabling semantic search and contextual grounding.\n",
        "\n",
        "Embeddings: We use HuggingFace’s all-MiniLM-L6-v2 (384-dim) to convert text chunks into dense numerical vectors.\n",
        "\n",
        "FAISS: An efficient vector similarity search library developed by Meta, designed for fast nearest-neighbor retrieval over dense embeddings without requiring external database infrastructure.\n",
        "\n",
        "LangChain Vector Store: Wraps FAISS to simplify storing and retrieving document embeddings."
      ],
      "metadata": {
        "id": "zs-Y34RpCbLM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OE7GVnbbJHif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize HuggingFace embedding model\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name = \"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Build a LangChain vector store wrapper on top of FAISS\n",
        "# This allows seamless integration between LangChain documents and HuggingFace embeddings\n",
        "vectorstore = FAISS.from_documents(\n",
        "    documents=chunks,\n",
        "    embedding= embeddings\n",
        ")"
      ],
      "metadata": {
        "id": "KVGh3WNkIRYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 7\n",
        "Build and Run the RAG Question-Answering Chain\n",
        "\n",
        "In this step, we connect the retriever, language model, and prompt into a working Retrieval-Augmented Generation (RAG) pipeline.\n",
        "\n",
        "The retriever fetches the most relevant chunks from FAISS, and the LLM uses them as context to answer the user’s question.\n",
        "\n",
        "Key Components:\n",
        "\n",
        "LLM: An open source model (mistralai/Mistral-7B-Instruct-v0.2) is used for no-cost, fast inference.\n",
        "\n",
        "Retriever: Fetches the top-k (here k=4) semantically closest chunks from the vector store.\n",
        "\n",
        "PromptTemplate: Guides the LLM to answer only using the retrieved context, avoiding hallucination.\n",
        "\n",
        "Chain: Combines retrieval, prompt formatting, and LLM inference into a single callable pipeline.\n",
        "\n",
        "At the end, a sample question is asked, and both the final answer and the retrieved chunks are displayed to show how the model grounded its response."
      ],
      "metadata": {
        "id": "9t1xls6xKeQ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YTYV_1b9NLq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "\n",
        "# Build the pipeleine\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model = \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "    temperature=0.001,\n",
        "    max_new_tokens=200,\n",
        "    return_full_text=False\n",
        ")\n",
        "# Initialize the llm\n",
        "llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "# Create a retriever from the FAISS vector store\n",
        "# It fetches the top 4 most relevant chunks per query\n",
        "retriever = vectorstore.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\"k\": 4})"
      ],
      "metadata": {
        "id": "_5GsL23LLwi6",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import utilities for prompt-based LLM chaining\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.schema import StrOutputParser\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# Define a strict prompt template\n",
        "template = \"\"\"\n",
        "You are a helpful assistant that answers using only the provided context.\n",
        "If the answer is not contained in the context, say \"I do not know\".\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Other requirements: Structure the answer in a way that is easy to read and understand.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=template\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GGT9nQwxOggD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper functions to format retrieved docs with their metadata for readability\n",
        "def format_docs(docs):\n",
        "  return \"\\n\\n\".join(\n",
        "      [f\"[{i+1} {d.page_content}\\n (meta: {d.metadata})\" for i, d in enumerate(docs)]\n",
        "  )\n",
        "\n",
        "  # Create the full retrieval-augmented generation chain\n",
        "chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    chain_type_kwargs={\"prompt\": prompt},\n",
        "    return_source_documents=True\n",
        ")"
      ],
      "metadata": {
        "id": "Eqs2FfTSUXcw"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example question that may match multiple documents\n",
        "question = \"Provide me a name of a student who can collaborate on a Robot and Air Quality project\"\n",
        "\n",
        "result = chain(question)\n",
        "\n",
        "print(result[\"result\"])\n"
      ],
      "metadata": {
        "id": "4-pn_siWYx7W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bac800e-ec29-47de-d5a5-4d052e1ecb36"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1970148065.py:4: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use invoke instead.\n",
            "  result = chain(question)\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Based on the context provided, Dr. Emily Zhang from the Robotics Engineering department and Dr. Patricia Brown from the Environmental Science department have students working on robotics and air quality projects respectively. A potential student for collaboration could be Kenji Tanaka (student ID: STU2024004) from Dr. Emily Zhang's lab, as his project involves autonomous delivery robots and obstacle avoidance, which could potentially be integrated with an air quality monitoring system. Alternatively, David Miller (student ID: STU2024008) from Dr. Patricia Brown's lab could also be a potential collaborator, as his project focuses on real-time air quality monitoring with predictive analytics, which could benefit from the integration of robotics technology.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Provide me names of three students who can collaborate on a Robot and Air Quality project\"\n",
        "\n",
        "result = chain(question)\n",
        "\n",
        "print(result[\"result\"])\n"
      ],
      "metadata": {
        "id": "uxAMSTK3xhgW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b999037a-f3af-40a2-f19e-358025ba41ae"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Based on the context provided, three students who can potentially collaborate on a Robot and Air Quality project are:\n",
            "\n",
            "1. Kenji Tanaka (Email: kenji.tanaka@university.edu) - He is a second-year student in the Robotics Engineering department, working on an autonomous delivery robot project under the supervision of Dr. Emily Zhang. His project involves computer vision and path planning, which could be beneficial for a Robot and Air Quality project.\n",
            "\n",
            "2. Dr. Michael Brown (Email: mbrown@university.edu) - Although he is a faculty member, his expertise in Internet of Things (IoT) and embedded systems could be valuable for developing the IoT infrastructure required to monitor and analyze air quality data.\n",
            "\n",
            "3. Ahmed Hassan (Email: ahmed.hassan@university.edu) - A third-year student in the Urban Planning & Computer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question to check if the model respects instructions"
      ],
      "metadata": {
        "id": "Ii-n4OBLrLwN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Who is the president of France ?\"\n",
        "\n",
        "result = chain(question)\n",
        "\n",
        "print(result[\"result\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMwTtT-bqXCN",
        "outputId": "4970d096-499a-4f5f-a26a-328b4b199d63"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: I do not know. The context provided does not mention anything about the president of France.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**An improved and complete pipeline is designed in another notebook with a streamlit application.**"
      ],
      "metadata": {
        "id": "zcofbrtHK8DQ"
      }
    }
  ]
}